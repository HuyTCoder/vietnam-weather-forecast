{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a7634c",
   "metadata": {},
   "source": [
    "## TCN (PyTorch) — sequences from fetch singlekeys (20 Locations)\n",
    "\n",
    "**Yêu cầu Dataset:**\n",
    "- Chạy notebook `fetch-demo-data-singlekeys.ipynb` trước (đã fetch 20 tỉnh/thành)\n",
    "- Upload output thành Kaggle Dataset\n",
    "- Add dataset vào notebook này\n",
    "\n",
    "**Dữ liệu format:**\n",
    "- Files theo **location_id** (UUID), không theo tên location\n",
    "- Ví dụ: `{location_id}_train_2021_2023_seq_multi_lag49_h100.npz`\n",
    "- Metadata: `weather_20loc/data/meta/locations.json` chứa mapping location_id -> name\n",
    "\n",
    "**Config:**\n",
    "- LAG = 49h lookback\n",
    "- HORIZON = 100h forecast (~4 ngày)\n",
    "- 20 locations thay vì 34/63\n",
    "- tcn_channels = 192 (giảm từ 256)\n",
    "- tcn_levels = 4 (giảm từ 5) - vẫn đủ receptive field (~61) cho 49h\n",
    "- batch_size = 96 (tối ưu cho T4 GPU)\n",
    "- AMP (Mixed Precision) để tăng tốc\n",
    "\n",
    "**Features:**\n",
    "- Accelerator: cpu / gpu\n",
    "- Auto-detect paths: tự dò SEQ_DIR\n",
    "- Report: bao gồm location_id column trong CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# train_tcn_weather_2step_full_v1p1.py\n",
    "# TCN multi-target weather + precip 2-step (event+amount)\n",
    "# 20 locations, LAG=49, HORIZON=100\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) ONE-LINE SWITCH\n",
    "# ============================================================\n",
    "ACCELERATOR = \"gpu\"   # \"gpu\" | \"cpu\"\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# 1) CONFIG (OPTIMIZED for 20 locations + Kaggle Free Tier)\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    base_dir: str = \"/kaggle/input/general-demo-data/weather_20loc\"\n",
    "    seq_dir_rel: str = \"data/sequences\"\n",
    "    meta_dir_rel: str = \"data/meta\"\n",
    "\n",
    "    lag: int = 49           # 49h lookback\n",
    "    horizon: int = 100      # 100h forecast (~4 days)\n",
    "\n",
    "    # ---- TCN model\n",
    "    # With levels=4, k=3, dilations 1..8, 2 conv/block -> receptive_field ~ 61\n",
    "    tcn_channels: int = 192     # Reduced from 256\n",
    "    tcn_levels: int = 4         # Reduced from 5 (still enough for 24h input)\n",
    "    kernel_size: int = 3\n",
    "    dropout: float = 0.15\n",
    "    use_weight_norm: bool = True\n",
    "    act: str = \"relu\"\n",
    "\n",
    "    # ---- train (OPTIMIZED)\n",
    "    epochs: int = 20            # Reduced from 25\n",
    "    batch_size: int = 96        # Optimized for T4 GPU\n",
    "    lr: float = 2e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    patience: int = 5\n",
    "\n",
    "    # ---- precip\n",
    "    rain_thr_mm: float = 0.1\n",
    "    amount_space: str = \"log1p\"\n",
    "    precip_mm_max: float = 200.0\n",
    "    loss_cls_w: float = 0.35\n",
    "    loss_reg_w: float = 0.65\n",
    "    pos_weight_cap: float = 20.0\n",
    "\n",
    "    # ---- stabilize multi-target loss\n",
    "    scale_y_other: bool = True\n",
    "\n",
    "    w_temp: float = 1.0\n",
    "    w_rh: float = 0.7\n",
    "    w_press: float = 0.5\n",
    "    w_cloud: float = 0.7\n",
    "    w_wind: float = 0.8\n",
    "\n",
    "    # ---- report\n",
    "    run_report: bool = True\n",
    "    p_thr_cand: Tuple[float, ...] = tuple(np.round(np.linspace(0.05, 0.95, 19), 2).tolist())\n",
    "    bins: Tuple[Tuple[int, int], ...] = ((1,24),(25,48),(49,72),(73,100))\n",
    "\n",
    "    # output\n",
    "    out_dir: str = \"/kaggle/working/tcn_weather_2step_out\"\n",
    "\n",
    "    run_id: str = \"\"\n",
    "    deterministic: bool = False\n",
    "    num_workers: int = 2\n",
    "    pin_memory: bool = True\n",
    "    save_preds: bool = True\n",
    "\n",
    "    train_split_key: str = \"train_2021_2023\"\n",
    "    val_split_key: str = \"val_2024\"\n",
    "    test_split_key: str = \"test_2025_01_to_2025_11\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "_env_run_id = os.environ.get(\"RUN_ID\", \"\").strip()\n",
    "if not cfg.run_id:\n",
    "    cfg.run_id = _env_run_id or time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUT_DIR = Path(cfg.out_dir) / cfg.run_id\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR = OUT_DIR / \"reports\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def is_gpu() -> bool:\n",
    "    return ACCELERATOR.lower() == \"gpu\"\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        if getattr(cfg, \"deterministic\", False):\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "# ============================================================\n",
    "# 2) DATA DISCOVERY + LOADING\n",
    "# ============================================================\n",
    "def find_base_dir() -> Path:\n",
    "    p = Path(cfg.base_dir)\n",
    "    if (p / cfg.seq_dir_rel).exists():\n",
    "        return p\n",
    "    root = Path(\"/kaggle/input\")\n",
    "    for pattern in [\"weather_20loc\", \"weather_34loc\", \"weather_63loc\", \"weather_4loc\"]:\n",
    "        hits = list(root.rglob(f\"{pattern}/data/sequences\"))\n",
    "        if hits:\n",
    "            return hits[0].parent.parent\n",
    "    hits = list(root.rglob(\"data/sequences\"))\n",
    "    if hits:\n",
    "        return hits[0].parent.parent\n",
    "    hits = list(root.rglob(f\"*_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"))\n",
    "    if hits:\n",
    "        return hits[0].parent.parent.parent\n",
    "    raise FileNotFoundError(\"Cannot find base dir in /kaggle/input\")\n",
    "\n",
    "BASE_DIR = find_base_dir()\n",
    "SEQ_DIR = BASE_DIR / cfg.seq_dir_rel\n",
    "META_DIR = BASE_DIR / cfg.meta_dir_rel\n",
    "print(\"[BASE_DIR]\", BASE_DIR)\n",
    "print(\"[SEQ_DIR]\", SEQ_DIR)\n",
    "print(\"[META_DIR]\", META_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 2.1) LOCATION ID LOADING\n",
    "# ============================================================\n",
    "def load_location_ids() -> Tuple[List[str], Dict[str, str]]:\n",
    "    meta_file = META_DIR / \"locations.json\"\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        loc_ids = meta.get(\"location_ids\", [])\n",
    "        loc_names = {loc[\"location_id\"]: loc[\"name\"] for loc in meta.get(\"locations\", [])}\n",
    "        print(f\"[meta] Loaded {len(loc_ids)} location_ids from {meta_file.name}\")\n",
    "        return loc_ids, loc_names\n",
    "    \n",
    "    pattern = f\"*_{cfg.train_split_key}_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"\n",
    "    files = sorted(SEQ_DIR.glob(pattern))\n",
    "    loc_ids = []\n",
    "    for fp in files:\n",
    "        loc_id = fp.name.split(f\"_{cfg.train_split_key}_\")[0]\n",
    "        if loc_id and loc_id not in loc_ids:\n",
    "            loc_ids.append(loc_id)\n",
    "    print(f\"[fallback] Found {len(loc_ids)} location_ids from file scan\")\n",
    "    return loc_ids, {lid: lid[:8] for lid in loc_ids}\n",
    "\n",
    "LOCATION_IDS, LOC_NAMES = load_location_ids()\n",
    "print(\"[LOCATION_IDS]\", LOCATION_IDS)\n",
    "for lid in LOCATION_IDS:\n",
    "    print(f\"  {lid} -> {LOC_NAMES.get(lid, '?')}\")\n",
    "\n",
    "def list_npz(split_key: str) -> List[Path]:\n",
    "    files = sorted(SEQ_DIR.glob(f\"*_{split_key}_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No npz for split={split_key} in {SEQ_DIR}\")\n",
    "    return files\n",
    "\n",
    "def get_loc_id_from_file(fp: Path) -> str:\n",
    "    name = fp.name\n",
    "    for split in [cfg.train_split_key, cfg.val_split_key, cfg.test_split_key]:\n",
    "        if f\"_{split}_\" in name:\n",
    "            return name.split(f\"_{split}_\")[0]\n",
    "    return name.split(\"_\")[0]\n",
    "\n",
    "TRAIN_FILES = list_npz(cfg.train_split_key)\n",
    "VAL_FILES   = list_npz(cfg.val_split_key)\n",
    "TEST_FILES  = list_npz(cfg.test_split_key)\n",
    "print(\"[files] train:\", len(TRAIN_FILES), \"val:\", len(VAL_FILES), \"test:\", len(TEST_FILES))\n",
    "\n",
    "def load_npz(fp: Path):\n",
    "    z = np.load(fp, allow_pickle=False)\n",
    "    X = z[\"X\"].astype(np.float32, copy=False)\n",
    "    Y = z[\"Y\"].astype(np.float32, copy=False)\n",
    "    Tm = z[\"T\"]\n",
    "    meta = json.loads(z[\"meta\"].item())\n",
    "    return X, Y, Tm, meta\n",
    "\n",
    "X0, Y0, T0, meta = load_npz(TRAIN_FILES[0])\n",
    "input_cols = meta.get(\"input_cols\") or meta.get(\"x_cols\")\n",
    "target_cols = meta.get(\"target_cols\") or meta.get(\"y_cols\")\n",
    "if input_cols is None or target_cols is None:\n",
    "    raise KeyError(f\"Meta missing input/target cols. Keys={list(meta.keys())}\")\n",
    "IDX = {c:i for i,c in enumerate(target_cols)}\n",
    "if \"precipitation\" not in IDX:\n",
    "    raise KeyError(f\"'precipitation' not found in target_cols: {target_cols}\")\n",
    "PRECIP_IDX = IDX[\"precipitation\"]\n",
    "\n",
    "n_feat = X0.shape[-1]\n",
    "n_tgt  = Y0.shape[-1]\n",
    "del X0, Y0, T0\n",
    "gc.collect()\n",
    "\n",
    "print(\"[meta] n_feat:\", n_feat, \"n_tgt:\", n_tgt)\n",
    "print(\"[targets]\", target_cols)\n",
    "print(\"[PRECIP_IDX]\", PRECIP_IDX, \"->\", target_cols[PRECIP_IDX])\n",
    "\n",
    "# ============================================================\n",
    "# 3) X SCALER STREAMING\n",
    "# ============================================================\n",
    "def compute_x_scaler_stream(files: List[Path], n_feat: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    s1 = np.zeros(n_feat, dtype=np.float64)\n",
    "    s2 = np.zeros(n_feat, dtype=np.float64)\n",
    "    n = 0\n",
    "\n",
    "    for fp in files:\n",
    "        X, _, _, _ = load_npz(fp)\n",
    "        x2 = X.reshape(-1, n_feat).astype(np.float64, copy=False)\n",
    "        s1 += x2.sum(axis=0)\n",
    "        s2 += (x2 * x2).sum(axis=0)\n",
    "        n += x2.shape[0]\n",
    "        del X, x2\n",
    "        gc.collect()\n",
    "\n",
    "    mu = (s1 / max(n,1)).astype(np.float32)\n",
    "    var = (s2 / max(n,1) - (mu.astype(np.float64) ** 2)).astype(np.float64)\n",
    "    var = np.maximum(var, 1e-6)\n",
    "    sd = np.sqrt(var).astype(np.float32)\n",
    "    sd = np.where(sd < 1e-6, 1.0, sd).astype(np.float32)\n",
    "    return mu, sd\n",
    "\n",
    "X_mu, X_sd = compute_x_scaler_stream(TRAIN_FILES, n_feat)\n",
    "np.savez(\n",
    "    OUT_DIR / \"x_scaler.npz\",\n",
    "    mu=X_mu, sd=X_sd, input_cols=np.array(input_cols, dtype=object),\n",
    ")\n",
    "\n",
    "def scale_x(X: np.ndarray) -> np.ndarray:\n",
    "    return ((X - X_mu[None,None,:]) / X_sd[None,None,:]).astype(np.float32, copy=False)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Y SCALER (non-precip only) STREAMING\n",
    "# ============================================================\n",
    "def compute_y_scaler_stream_nonprecip(files: List[Path], n_tgt: int, precip_idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    s1 = np.zeros(n_tgt, dtype=np.float64)\n",
    "    s2 = np.zeros(n_tgt, dtype=np.float64)\n",
    "    n = 0\n",
    "\n",
    "    for fp in files:\n",
    "        _, Y, _, _ = load_npz(fp)\n",
    "        y2 = Y.reshape(-1, n_tgt).astype(np.float64, copy=False)\n",
    "        s1 += y2.sum(axis=0)\n",
    "        s2 += (y2 * y2).sum(axis=0)\n",
    "        n += y2.shape[0]\n",
    "        del Y, y2\n",
    "        gc.collect()\n",
    "\n",
    "    mu = (s1 / max(n,1)).astype(np.float32)\n",
    "    var = (s2 / max(n,1) - (mu.astype(np.float64) ** 2)).astype(np.float64)\n",
    "    var = np.maximum(var, 1e-6)\n",
    "    sd = np.sqrt(var).astype(np.float32)\n",
    "    sd = np.where(sd < 1e-6, 1.0, sd).astype(np.float32)\n",
    "\n",
    "    mu[precip_idx] = 0.0\n",
    "    sd[precip_idx] = 1.0\n",
    "    return mu, sd\n",
    "\n",
    "if cfg.scale_y_other:\n",
    "    Y_mu, Y_sd = compute_y_scaler_stream_nonprecip(TRAIN_FILES, n_tgt, PRECIP_IDX)\n",
    "    np.savez(\n",
    "        OUT_DIR / \"y_scaler_nonprecip.npz\",\n",
    "        mu=Y_mu, sd=Y_sd, target_cols=np.array(target_cols, dtype=object),\n",
    "    )\n",
    "else:\n",
    "    Y_mu = np.zeros(n_tgt, dtype=np.float32)\n",
    "    Y_sd = np.ones(n_tgt, dtype=np.float32)\n",
    "\n",
    "# ============================================================\n",
    "# 5) POS_WEIGHT for precip event\n",
    "# ============================================================\n",
    "def compute_pos_weight_stream(files: List[Path], precip_idx: int, thr_mm: float, cap: float) -> float:\n",
    "    pos = 0.0\n",
    "    total = 0.0\n",
    "    for fp in files:\n",
    "        _, Y, _, _ = load_npz(fp)\n",
    "        y = Y[..., precip_idx]\n",
    "        pos += float((y >= thr_mm).sum())\n",
    "        total += float(y.size)\n",
    "        del Y, y\n",
    "        gc.collect()\n",
    "    neg = total - pos\n",
    "    pw = neg / (pos + 1e-9)\n",
    "    pw = float(min(pw, cap))\n",
    "    return pw\n",
    "\n",
    "POS_WEIGHT = compute_pos_weight_stream(TRAIN_FILES, PRECIP_IDX, cfg.rain_thr_mm, cfg.pos_weight_cap)\n",
    "print(\"[pos_weight]\", POS_WEIGHT)\n",
    "\n",
    "# ============================================================\n",
    "# 6) TORCH + MODEL (TCN)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"[torch]\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = (not cfg.deterministic)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def maybe_weight_norm(conv: nn.Module, enabled: bool) -> nn.Module:\n",
    "    if not enabled:\n",
    "        return conv\n",
    "    try:\n",
    "        from torch.nn.utils.parametrizations import weight_norm as wn\n",
    "        return wn(conv)\n",
    "    except Exception:\n",
    "        try:\n",
    "            from torch.nn.utils import weight_norm as wn2\n",
    "            return wn2(conv)\n",
    "        except Exception:\n",
    "            return conv\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
    "\n",
    "def precip_log_to_mm(pred_log: torch.Tensor, mm_max: float) -> torch.Tensor:\n",
    "    return torch.expm1(pred_log).clamp(min=0.0, max=float(mm_max))\n",
    "\n",
    "def _act(name: str):\n",
    "    name = (name or \"relu\").lower()\n",
    "    if name == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    return nn.ReLU()\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, kernel_size: int, dilation: int, use_weight_norm: bool):\n",
    "        super().__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        conv = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, dilation=dilation)\n",
    "        self.conv = maybe_weight_norm(conv, use_weight_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, k: int, d: int, dropout: float, use_weight_norm: bool, act_name: str):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size=k, dilation=d, use_weight_norm=use_weight_norm)\n",
    "        self.act1  = _act(act_name)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size=k, dilation=d, use_weight_norm=use_weight_norm)\n",
    "        self.act2  = _act(act_name)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.down = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else None\n",
    "        self.out_act = _act(act_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.drop1(self.act1(self.conv1(x)))\n",
    "        y = self.drop2(self.act2(self.conv2(y)))\n",
    "        res = x if self.down is None else self.down(x)\n",
    "        return self.out_act(y + res)\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, in_ch: int, hidden: int, levels: int, k: int, dropout: float, use_weight_norm: bool, act_name: str):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        ch_in = in_ch\n",
    "        for i in range(levels):\n",
    "            d = 2 ** i\n",
    "            blocks.append(TCNBlock(\n",
    "                ch_in, hidden, k=k, d=d, dropout=dropout,\n",
    "                use_weight_norm=use_weight_norm, act_name=act_name\n",
    "            ))\n",
    "            ch_in = hidden\n",
    "        self.net = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TCNWeather2Step(nn.Module):\n",
    "    def __init__(self, n_feat, n_tgt, horizon, hidden, levels, k, dropout, precip_idx,\n",
    "                 amount_space=\"log1p\", use_weight_norm=True, act_name=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_tgt = n_tgt\n",
    "        self.precip_idx = precip_idx\n",
    "        self.amount_space = amount_space\n",
    "\n",
    "        self.enc = TCNEncoder(\n",
    "            in_ch=n_feat, hidden=hidden, levels=levels, k=k,\n",
    "            dropout=dropout, use_weight_norm=use_weight_norm, act_name=act_name\n",
    "        )\n",
    "        self.head_main = nn.Linear(hidden, horizon * n_tgt)\n",
    "        self.head_rain = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        h = self.enc(x)\n",
    "        h_last = h[:, :, -1]\n",
    "        y = self.head_main(h_last).view(-1, self.horizon, self.n_tgt)\n",
    "        rain_logit = self.head_rain(h_last).view(-1, self.horizon)\n",
    "\n",
    "        if self.amount_space == \"mm\":\n",
    "            y[..., self.precip_idx] = F.softplus(y[..., self.precip_idx])\n",
    "        return y, rain_logit\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def receptive_field(levels: int, k: int, convs_per_block: int = 2) -> int:\n",
    "    dilations = [2**i for i in range(levels)]\n",
    "    rf = 1 + (k - 1) * convs_per_block * sum(dilations)\n",
    "    return int(rf)\n",
    "\n",
    "def build_optimizer(model: nn.Module):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "def safe_set_weight(w: torch.Tensor, name: str, value: float):\n",
    "    if name in IDX:\n",
    "        w[..., IDX[name]] = float(value)\n",
    "\n",
    "def build_target_weights(device, dtype=torch.float32) -> torch.Tensor:\n",
    "    w = torch.ones((1, 1, n_tgt), device=device, dtype=dtype)\n",
    "    w[..., PRECIP_IDX] = 0.0\n",
    "\n",
    "    if cfg.scale_y_other:\n",
    "        return w\n",
    "\n",
    "    safe_set_weight(w, \"temperature_2m\", cfg.w_temp)\n",
    "    safe_set_weight(w, \"relative_humidity_2m\", cfg.w_rh)\n",
    "    safe_set_weight(w, \"surface_pressure\", cfg.w_press)\n",
    "    safe_set_weight(w, \"cloud_cover\", cfg.w_cloud)\n",
    "    safe_set_weight(w, \"u10\", cfg.w_wind)\n",
    "    safe_set_weight(w, \"v10\", cfg.w_wind)\n",
    "    return w\n",
    "\n",
    "def loss_fn(y_pred, rain_logit, y_true, pos_weight_value: float,\n",
    "            y_mu_t: torch.Tensor, y_sd_t: torch.Tensor,\n",
    "            w_tgt: torch.Tensor):\n",
    "    if cfg.scale_y_other:\n",
    "        yp_s = (y_pred - y_mu_t[None,None,:]) / y_sd_t[None,None,:]\n",
    "        yt_s = (y_true - y_mu_t[None,None,:]) / y_sd_t[None,None,:]\n",
    "    else:\n",
    "        yp_s, yt_s = y_pred, y_true\n",
    "\n",
    "    reg = F.smooth_l1_loss(yp_s, yt_s, reduction=\"none\")\n",
    "    loss_other = (reg * w_tgt).mean()\n",
    "\n",
    "    y_mm = y_true[..., PRECIP_IDX]\n",
    "    rain_label = (y_mm >= cfg.rain_thr_mm).float()\n",
    "    pw = torch.tensor(pos_weight_value, device=y_true.device, dtype=y_true.dtype)\n",
    "    loss_cls = F.binary_cross_entropy_with_logits(rain_logit, rain_label, pos_weight=pw)\n",
    "\n",
    "    pos = rain_label > 0.5\n",
    "    if cfg.amount_space == \"mm\":\n",
    "        pred_mm = y_pred[..., PRECIP_IDX].clamp(min=0.0, max=float(cfg.precip_mm_max))\n",
    "        loss_reg = F.smooth_l1_loss(pred_mm[pos], y_mm[pos]) if pos.any() else pred_mm.mean() * 0.0\n",
    "    else:\n",
    "        pred_log = y_pred[..., PRECIP_IDX]\n",
    "        true_log = torch.log1p(y_mm.clamp(min=0.0))\n",
    "        loss_reg = F.smooth_l1_loss(pred_log[pos], true_log[pos]) if pos.any() else pred_log.mean() * 0.0\n",
    "\n",
    "    total = loss_other + cfg.loss_cls_w * loss_cls + cfg.loss_reg_w * loss_reg\n",
    "    return total\n",
    "\n",
    "# ============================================================\n",
    "# 7) SANITY CHECKS\n",
    "# ============================================================\n",
    "def sanity_checks():\n",
    "    X, Y, _, _ = load_npz(TRAIN_FILES[0])\n",
    "    assert X.shape[1] == cfg.lag, f\"X lag mismatch: {X.shape}\"\n",
    "    assert Y.shape[1] == cfg.horizon, f\"Y horizon mismatch: {Y.shape}\"\n",
    "    assert X.shape[2] == n_feat, f\"n_feat mismatch: {X.shape}\"\n",
    "    assert Y.shape[2] == n_tgt, f\"n_tgt mismatch: {Y.shape}\"\n",
    "\n",
    "    pmin = float(np.min(Y[..., PRECIP_IDX]))\n",
    "    if pmin < -1e-6:\n",
    "        raise ValueError(f\"Precip appears negative (min={pmin}).\")\n",
    "    Xs = scale_x(X[: min(2048, X.shape[0])])\n",
    "    if not np.isfinite(Xs).all():\n",
    "        raise ValueError(\"Scaled X contains NaN/Inf.\")\n",
    "    Ys = Y[: min(2048, Y.shape[0])]\n",
    "    if not np.isfinite(Ys).all():\n",
    "        raise ValueError(\"Y contains NaN/Inf.\")\n",
    "\n",
    "    print(\"[sanity] OK | precip_min_mm:\", pmin, \"| X_scaled finite:\", True)\n",
    "    del X, Y, Xs, Ys\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# 8) TRAIN (GPU/CPU) — streaming over files\n",
    "# ============================================================\n",
    "def train_gpu_cpu():\n",
    "    sanity_checks()\n",
    "\n",
    "    device = torch.device(\"cuda\" if (is_gpu() and torch.cuda.is_available()) else \"cpu\")\n",
    "    print(\"[device]\", device)\n",
    "\n",
    "    rf = receptive_field(cfg.tcn_levels, cfg.kernel_size, convs_per_block=2)\n",
    "    print(f\"[TCN] levels={cfg.tcn_levels} k={cfg.kernel_size} receptive_field≈{rf} (input_len={cfg.lag})\")\n",
    "\n",
    "    model = TCNWeather2Step(\n",
    "        n_feat=n_feat, n_tgt=n_tgt, horizon=cfg.horizon,\n",
    "        hidden=cfg.tcn_channels, levels=cfg.tcn_levels, k=cfg.kernel_size,\n",
    "        dropout=cfg.dropout, precip_idx=PRECIP_IDX, amount_space=cfg.amount_space,\n",
    "        use_weight_norm=cfg.use_weight_norm, act_name=cfg.act\n",
    "    ).to(device)\n",
    "    print(\"[model] params:\", count_params(model))\n",
    "\n",
    "    opt = build_optimizer(model)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    y_mu_t = torch.tensor(Y_mu, device=device, dtype=torch.float32)\n",
    "    y_sd_t = torch.tensor(Y_sd, device=device, dtype=torch.float32)\n",
    "    w_tgt  = build_target_weights(device=device, dtype=torch.float32)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_path = OUT_DIR / \"tcn_weather2step_best.pt\"\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        tr_sum = 0.0\n",
    "        tr_n = 0\n",
    "\n",
    "        files = TRAIN_FILES.copy()\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "        for fp in files:\n",
    "            X, Y, _, _ = load_npz(fp)\n",
    "            X = scale_x(X)\n",
    "\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(\n",
    "                ds,\n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=(2 if device.type == \"cuda\" else 0),\n",
    "                pin_memory=(device.type == \"cuda\"),\n",
    "                drop_last=True\n",
    "            )\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                        yp, rl = model(xb)\n",
    "                        loss = loss_fn(yp, rl, yb, POS_WEIGHT, y_mu_t, y_sd_t, w_tgt)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                        scaler.unscale_(opt)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    yp, rl = model(xb)\n",
    "                    loss = loss_fn(yp, rl, yb, POS_WEIGHT, y_mu_t, y_sd_t, w_tgt)\n",
    "                    loss.backward()\n",
    "                    if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                    opt.step()\n",
    "\n",
    "                tr_sum += float(loss.item()) * xb.size(0)\n",
    "                tr_n += xb.size(0)\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "        tr_loss = tr_sum / max(tr_n, 1)\n",
    "\n",
    "        model.eval()\n",
    "        va_sum = 0.0\n",
    "        va_n = 0\n",
    "        with torch.no_grad():\n",
    "            for fp in VAL_FILES:\n",
    "                X, Y, _, _ = load_npz(fp)\n",
    "                X = scale_x(X)\n",
    "\n",
    "                ds = ArrayDataset(X, Y)\n",
    "                loader = DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False)\n",
    "\n",
    "                for xb, yb in loader:\n",
    "                    xb = xb.to(device, non_blocking=True)\n",
    "                    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                            yp, rl = model(xb)\n",
    "                            loss = loss_fn(yp, rl, yb, POS_WEIGHT, y_mu_t, y_sd_t, w_tgt)\n",
    "                    else:\n",
    "                        yp, rl = model(xb)\n",
    "                        loss = loss_fn(yp, rl, yb, POS_WEIGHT, y_mu_t, y_sd_t, w_tgt)\n",
    "\n",
    "                    va_sum += float(loss.item()) * xb.size(0)\n",
    "                    va_n += xb.size(0)\n",
    "\n",
    "                del X, Y, ds, loader\n",
    "                gc.collect()\n",
    "\n",
    "        va_loss = va_sum / max(va_n, 1)\n",
    "        sched.step(va_loss)\n",
    "\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"epoch {ep:02d} | lr {lr_now:.2e} | train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
    "\n",
    "        if va_loss < best_val - 1e-6:\n",
    "            best_val = va_loss\n",
    "            bad = 0\n",
    "            ckpt_meta = dict(meta)\n",
    "            ckpt_meta.update({\n",
    "                \"n_feat\": n_feat,\n",
    "                \"n_tgt\": n_tgt,\n",
    "                \"precip_idx\": PRECIP_IDX,\n",
    "                \"input_cols\": input_cols,\n",
    "                \"target_cols\": target_cols,\n",
    "            })\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"cfg\": cfg.__dict__,\n",
    "                    \"meta\": ckpt_meta,\n",
    "                    \"pos_weight\": POS_WEIGHT,\n",
    "                    \"x_scaler\": {\"mu\": X_mu.tolist(), \"sd\": X_sd.tolist()},\n",
    "                    \"y_scaler_nonprecip\": {\"mu\": Y_mu.tolist(), \"sd\": Y_sd.tolist()},\n",
    "                },\n",
    "                best_path\n",
    "            )\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= cfg.patience:\n",
    "                print(\"[early stop]\")\n",
    "                break\n",
    "\n",
    "    print(\"[best val]\", best_val, \"->\", best_path)\n",
    "    return best_path\n",
    "\n",
    "# ============================================================\n",
    "# 9) REPORT\n",
    "# ============================================================\n",
    "def event_metrics_counts(y_true01: np.ndarray, y_pred01: np.ndarray):\n",
    "    tp = int(((y_true01 == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true01 == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true01 == 1) & (y_pred01 == 0)).sum())\n",
    "    return tp, fp, fn\n",
    "\n",
    "def tune_pthr_on_val(model, device) -> Tuple[float, pd.DataFrame]:\n",
    "    cand = np.array(cfg.p_thr_cand, dtype=np.float32)\n",
    "    K = len(cand)\n",
    "    tp = np.zeros(K, dtype=np.int64)\n",
    "    fp = np.zeros(K, dtype=np.int64)\n",
    "    fn = np.zeros(K, dtype=np.int64)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in VAL_FILES:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                _, logit = model(xb)\n",
    "                p = torch.sigmoid(logit).detach().cpu().numpy().astype(np.float32)\n",
    "                y_mm = yb[..., PRECIP_IDX].detach().cpu().numpy().astype(np.float32)\n",
    "                y_ev = (y_mm >= cfg.rain_thr_mm).reshape(-1)\n",
    "                p1 = p.reshape(-1)\n",
    "\n",
    "                for i, thr in enumerate(cand):\n",
    "                    pred = (p1 >= thr)\n",
    "                    tpi, fpi, fni = event_metrics_counts(y_ev.astype(np.int32), pred.astype(np.int32))\n",
    "                    tp[i] += tpi; fp[i] += fpi; fn[i] += fni\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "    best_i = int(np.argmax(f1))\n",
    "    best_thr = float(cand[best_i])\n",
    "\n",
    "    rank = pd.DataFrame({\n",
    "        \"p_thr\": cand,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn\n",
    "    }).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return best_thr, rank\n",
    "\n",
    "def accumulate_precip_stats(model, device, files: List[Path], p_thr: float, split_name: str, amount_space: str):\n",
    "    H = cfg.horizon\n",
    "    tp = np.zeros(H, dtype=np.int64)\n",
    "    fp = np.zeros(H, dtype=np.int64)\n",
    "    fn = np.zeros(H, dtype=np.int64)\n",
    "    npos = np.zeros(H, dtype=np.int64)\n",
    "\n",
    "    abs_hard = np.zeros(H, dtype=np.float64)\n",
    "    sq_hard  = np.zeros(H, dtype=np.float64)\n",
    "\n",
    "    n_all = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in files:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                yp, logit = model(xb)\n",
    "                p = torch.sigmoid(logit)\n",
    "\n",
    "                y_mm = yb[..., PRECIP_IDX]\n",
    "                y_mm_np = y_mm.detach().cpu().numpy().astype(np.float32)\n",
    "                y_ev_np = (y_mm_np >= cfg.rain_thr_mm)\n",
    "\n",
    "                if amount_space == \"mm\":\n",
    "                    pred_mm = yp[..., PRECIP_IDX].clamp(min=0.0, max=float(cfg.precip_mm_max))\n",
    "                else:\n",
    "                    pred_log = yp[..., PRECIP_IDX]\n",
    "                    pred_mm = precip_log_to_mm(pred_log, cfg.precip_mm_max)\n",
    "\n",
    "                pred_mm_np = pred_mm.detach().cpu().numpy().astype(np.float32)\n",
    "                p_np = p.detach().cpu().numpy().astype(np.float32)\n",
    "                pred_ev_np = (p_np >= p_thr)\n",
    "\n",
    "                tp += (pred_ev_np & y_ev_np).sum(axis=0).astype(np.int64)\n",
    "                fp += (pred_ev_np & (~y_ev_np)).sum(axis=0).astype(np.int64)\n",
    "                fn += ((~pred_ev_np) & y_ev_np).sum(axis=0).astype(np.int64)\n",
    "                npos += y_ev_np.sum(axis=0).astype(np.int64)\n",
    "\n",
    "                hard = np.where(pred_ev_np, pred_mm_np, 0.0).astype(np.float32)\n",
    "                err_h = hard - y_mm_np\n",
    "\n",
    "                abs_hard += np.abs(err_h).sum(axis=0)\n",
    "                sq_hard  += (err_h * err_h).sum(axis=0)\n",
    "\n",
    "                n_all += y_mm_np.shape[0]\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "    denom_all = max(n_all, 1)\n",
    "    mae_hard = abs_hard / denom_all\n",
    "    rmse_hard = np.sqrt(sq_hard / denom_all)\n",
    "\n",
    "    rep = pd.DataFrame({\n",
    "        \"split\": split_name,\n",
    "        \"horizon\": np.arange(1, H+1),\n",
    "        \"rain_thr_mm\": cfg.rain_thr_mm,\n",
    "        \"p_thr\": float(p_thr),\n",
    "        \"event_precision\": prec,\n",
    "        \"event_recall\": rec,\n",
    "        \"event_f1\": f1,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "        \"npos\": npos,\n",
    "        \"final_mae_all_hard\": mae_hard,\n",
    "        \"final_rmse_all_hard\": rmse_hard,\n",
    "    })\n",
    "    return rep\n",
    "\n",
    "def accumulate_other_targets(model, device, files: List[Path], split_name: str):\n",
    "    H = cfg.horizon\n",
    "    T = len(target_cols)\n",
    "    targets = [t for t in range(T) if t != PRECIP_IDX]\n",
    "\n",
    "    sum_abs = np.zeros((len(targets), H), dtype=np.float64)\n",
    "    sum_sq  = np.zeros((len(targets), H), dtype=np.float64)\n",
    "    n_all = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in files:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                yp, _ = model(xb)\n",
    "\n",
    "                y_true = yb.detach().cpu().numpy().astype(np.float32)\n",
    "                y_pred = yp.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "                for i, ti in enumerate(targets):\n",
    "                    de = (y_pred[:, :, ti] - y_true[:, :, ti]).astype(np.float64)\n",
    "                    sum_abs[i] += np.abs(de).sum(axis=0)\n",
    "                    sum_sq[i]  += (de * de).sum(axis=0)\n",
    "\n",
    "                n_all += y_true.shape[0]\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    denom = max(n_all, 1)\n",
    "    rows = []\n",
    "    for i, ti in enumerate(targets):\n",
    "        name = target_cols[ti]\n",
    "        mae = (sum_abs[i] / denom).astype(np.float64)\n",
    "        rmse = np.sqrt(sum_sq[i] / denom).astype(np.float64)\n",
    "        for h in range(H):\n",
    "            rows.append({\n",
    "                \"split\": split_name,\n",
    "                \"target\": name,\n",
    "                \"horizon\": h+1,\n",
    "                \"mae\": float(mae[h]),\n",
    "                \"rmse\": float(rmse[h]),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_bins_precip(rep_split: pd.DataFrame):\n",
    "    rows = []\n",
    "    for a,b in cfg.bins:\n",
    "        r = rep_split[(rep_split[\"horizon\"]>=a) & (rep_split[\"horizon\"]<=b)]\n",
    "        if len(r) == 0:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"split\": r[\"split\"].iloc[0],\n",
    "            \"horizon_bin\": f\"{a}-{b}\",\n",
    "            \"event_f1_mean\": float(r[\"event_f1\"].mean()),\n",
    "            \"event_recall_mean\": float(r[\"event_recall\"].mean()),\n",
    "            \"event_precision_mean\": float(r[\"event_precision\"].mean()),\n",
    "            \"mae_all_hard_mean\": float(r[\"final_mae_all_hard\"].mean()),\n",
    "            \"rmse_all_hard_mean\": float(r[\"final_rmse_all_hard\"].mean()),\n",
    "            \"npos_sum\": int(r[\"npos\"].sum()),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_bins_other(rep_long: pd.DataFrame):\n",
    "    rows = []\n",
    "    for (split, target), g in rep_long.groupby([\"split\", \"target\"]):\n",
    "        for a,b in cfg.bins:\n",
    "            r = g[(g[\"horizon\"]>=a) & (g[\"horizon\"]<=b)]\n",
    "            if len(r) == 0:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"split\": split,\n",
    "                \"target\": target,\n",
    "                \"horizon_bin\": f\"{a}-{b}\",\n",
    "                \"mae_mean\": float(r[\"mae\"].mean()),\n",
    "                \"rmse_mean\": float(r[\"rmse\"].mean()),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def run_report(best_ckpt_path: Path):\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    cfg_ck = ckpt.get(\"cfg\", {})\n",
    "    amount_space = str(cfg_ck.get(\"amount_space\", cfg.amount_space))\n",
    "\n",
    "    model = TCNWeather2Step(\n",
    "        n_feat=n_feat, n_tgt=n_tgt, horizon=cfg.horizon,\n",
    "        hidden=int(cfg_ck.get(\"tcn_channels\", cfg.tcn_channels)),\n",
    "        levels=int(cfg_ck.get(\"tcn_levels\", cfg.tcn_levels)),\n",
    "        k=int(cfg_ck.get(\"kernel_size\", cfg.kernel_size)),\n",
    "        dropout=float(cfg_ck.get(\"dropout\", cfg.dropout)),\n",
    "        precip_idx=PRECIP_IDX,\n",
    "        amount_space=amount_space,\n",
    "        use_weight_norm=bool(cfg_ck.get(\"use_weight_norm\", cfg.use_weight_norm)),\n",
    "        act_name=str(cfg_ck.get(\"act\", cfg.act)),\n",
    "    )\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    best_p_thr, thr_rank = tune_pthr_on_val(model, device)\n",
    "    thr_rank.to_csv(REPORT_DIR / \"precip_thr_tuning_val.csv\", index=False)\n",
    "    print(\"[REPORT] best_p_thr (VAL F1) =\", best_p_thr)\n",
    "\n",
    "    rep_p_val = accumulate_precip_stats(model, device, VAL_FILES, best_p_thr, \"val\", amount_space)\n",
    "    rep_p_te  = accumulate_precip_stats(model, device, TEST_FILES, best_p_thr, \"test\", amount_space)\n",
    "    rep_p = pd.concat([rep_p_val, rep_p_te], ignore_index=True)\n",
    "    rep_p.to_csv(REPORT_DIR / f\"tcn_precip_report_val_test_h1-{cfg.horizon}.csv\", index=False)\n",
    "\n",
    "    sum_p = pd.concat([summarize_bins_precip(rep_p_val), summarize_bins_precip(rep_p_te)], ignore_index=True)\n",
    "    sum_p[\"p_thr\"] = best_p_thr\n",
    "    sum_p[\"rain_thr_mm\"] = cfg.rain_thr_mm\n",
    "    sum_p.to_csv(REPORT_DIR / \"tcn_precip_summary_bins_val_test.csv\", index=False)\n",
    "\n",
    "    rep_o_val = accumulate_other_targets(model, device, VAL_FILES, \"val\")\n",
    "    rep_o_te  = accumulate_other_targets(model, device, TEST_FILES, \"test\")\n",
    "    rep_o = pd.concat([rep_o_val, rep_o_te], ignore_index=True)\n",
    "    rep_o.to_csv(REPORT_DIR / \"tcn_other_targets_report_val_test_long.csv\", index=False)\n",
    "\n",
    "    sum_o = summarize_bins_other(rep_o)\n",
    "    sum_o.to_csv(REPORT_DIR / \"tcn_other_targets_summary_bins_val_test.csv\", index=False)\n",
    "\n",
    "    print(\"[REPORT] saved in:\", REPORT_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 10) MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"ACCELERATOR:\", ACCELERATOR)\n",
    "    print(f\"CONFIG: LAG={cfg.lag}, HORIZON={cfg.horizon}, TCN_CHANNELS={cfg.tcn_channels}, LEVELS={cfg.tcn_levels}\")\n",
    "    best_ckpt = train_gpu_cpu()\n",
    "    if cfg.run_report:\n",
    "        run_report(best_ckpt)\n",
    "    print(\"DONE. out_dir =\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
