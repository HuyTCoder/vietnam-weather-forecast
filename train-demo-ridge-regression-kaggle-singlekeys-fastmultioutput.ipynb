{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7c97d7",
   "metadata": {},
   "source": [
    "## Ridge Regression (Kaggle) — Train by LOCATION_ID (20 Locations)\n",
    "\n",
    "**Yêu cầu Dataset:**\n",
    "- Chạy `fetch-demo-data-singlekeys.ipynb` trước (đã fetch 20 tỉnh/thành)\n",
    "- Upload output thành Kaggle Dataset\n",
    "- Add dataset vào notebook này\n",
    "\n",
    "**Config:**\n",
    "- LAG = 49h lookback\n",
    "- HORIZON = 100h forecast (~4 ngày)\n",
    "- 20 locations thay vì 34/63\n",
    "- Ridge là lightweight, có thể chạy all locations cùng lúc\n",
    "\n",
    "**Features:**\n",
    "- Có thể chạy nhiều targets\n",
    "- Tự dò TAB_DIR và load location_ids từ metadata\n",
    "- Fast multi-output: fit 1 lần cho tất cả horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ridge Regression trainer - Train by LOCATION_ID from Kaggle Dataset\n",
    "# 20 provinces/cities, LAG=49, HORIZON=100\n",
    "# ============================================================\n",
    "\n",
    "%pip install -q \"scikit-learn==1.8.0\"\n",
    "\n",
    "import os, json, gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import joblib\n",
    "\n",
    "# ============================================================\n",
    "# 0) RUN CONTROL (OPTIMIZED for 20 locations)\n",
    "# ============================================================\n",
    "TARGETS = [\"temp\", \"rain\", \"u10\", \"v10\", \"rh\", \"press\", \"cloud\"]\n",
    "\n",
    "LAG = 49            # 49h lookback\n",
    "H   = 100           # 100h forecast\n",
    "H_START = 1\n",
    "H_END   = 100\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "START_LOC_IDX = 0\n",
    "END_LOC_IDX = -1    # -1 = all remaining\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": \"train_2021_2023\",\n",
    "    \"val\":   \"val_2024\",\n",
    "    \"test\":  \"test_2025_01_to_2025_11\",\n",
    "}\n",
    "\n",
    "ALPHA = 1.0\n",
    "FIT_INTERCEPT = True\n",
    "RIDGE_SOLVER = \"auto\"\n",
    "RIDGE_TOL = 1e-4\n",
    "CLIP_NONNEG_FOR_RAIN = True\n",
    "SEED = 42\n",
    "\n",
    "# ============================================================\n",
    "# 1) AUTO-DETECT DATA DIR + LOAD LOCATION_IDS\n",
    "# ============================================================\n",
    "INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "\n",
    "def find_data_dir():\n",
    "    for pattern in [\"weather_20loc/data\", \"weather_34loc/data\", \"weather_63loc/data\", \"weather_4loc/data\"]:\n",
    "        for p in INPUT_ROOT.rglob(pattern):\n",
    "            if p.is_dir():\n",
    "                return p\n",
    "    for p in INPUT_ROOT.rglob(\"data/tabular\"):\n",
    "        if p.is_dir():\n",
    "            return p.parent\n",
    "    raise FileNotFoundError(\"Không tìm thấy data directory\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n",
    "TAB_DIR = DATA_DIR / \"tabular\"\n",
    "META_DIR = DATA_DIR / \"meta\"\n",
    "\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"TAB_DIR = {TAB_DIR}\")\n",
    "\n",
    "def load_location_ids():\n",
    "    meta_file = META_DIR / \"locations.json\"\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as f:\n",
    "            meta = json.load(f)\n",
    "        loc_ids = meta.get(\"location_ids\", [])\n",
    "        locations = meta.get(\"locations\", [])\n",
    "        print(f\"Loaded {len(loc_ids)} locations:\")\n",
    "        for loc in locations:\n",
    "            print(f\"  {loc['name']:15s} = {loc['location_id']}\")\n",
    "        return loc_ids, {loc[\"location_id\"]: loc[\"name\"] for loc in locations}\n",
    "    \n",
    "    # Fallback: scan files\n",
    "    files = list(TAB_DIR.glob(f\"*_{SPLITS['train']}_tab_temp_lag{LAG}_h{H}.parquet\"))\n",
    "    loc_ids = sorted(set(f.name.split(\"_\")[0] for f in files))\n",
    "    print(f\"Found {len(loc_ids)} location_ids from files\")\n",
    "    return loc_ids, {}\n",
    "\n",
    "LOCATION_IDS_ALL, LOC_NAMES = load_location_ids()\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "_start = START_LOC_IDX\n",
    "_end = END_LOC_IDX if END_LOC_IDX >= 0 else len(LOCATION_IDS_ALL)\n",
    "LOCATION_IDS = LOCATION_IDS_ALL[_start:_end]\n",
    "print(f\"[LOCATION BATCH] Using {len(LOCATION_IDS)}/{len(LOCATION_IDS_ALL)} locations (idx {_start}:{_end})\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) OUTPUT DIRS\n",
    "# ============================================================\n",
    "OUT_DIR = Path(\"/kaggle/working/ridge_out_singlekeys_fast\")\n",
    "MODEL_DIR = OUT_DIR / \"models\"\n",
    "REPORT_DIR = OUT_DIR / \"reports\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 3) IO HELPERS\n",
    "# ============================================================\n",
    "def ycol(h: int) -> str:\n",
    "    return f\"y_t+{h:03d}\"\n",
    "\n",
    "def file_path(loc_id: str, split_name: str, target_key: str) -> Path:\n",
    "    return TAB_DIR / f\"{loc_id}_{split_name}_tab_{target_key}_lag{LAG}_h{H}.parquet\"\n",
    "\n",
    "def loc_short_name(loc_id: str) -> str:\n",
    "    return LOC_NAMES.get(loc_id, loc_id[:8])\n",
    "\n",
    "def get_schema_cols(path: Path):\n",
    "    try:\n",
    "        import pyarrow.parquet as pq\n",
    "        return pq.ParquetFile(path).schema.names\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\", columns=None).columns.tolist()\n",
    "\n",
    "def load_XY_all(loc_id: str, split_name: str, target_key: str):\n",
    "    path = file_path(loc_id, split_name, target_key)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {path}\")\n",
    "\n",
    "    cols = get_schema_cols(path)\n",
    "    feat_cols = [c for c in cols if \"_lag\" in c]\n",
    "    y_cols = [ycol(h) for h in range(1, H+1)]\n",
    "\n",
    "    missing_y = [c for c in y_cols if c not in cols]\n",
    "    if missing_y:\n",
    "        raise ValueError(f\"{path.name}: missing {len(missing_y)} y columns\")\n",
    "\n",
    "    use_cols = feat_cols + y_cols\n",
    "    df = pd.read_parquet(path, columns=use_cols)\n",
    "\n",
    "    X = df[feat_cols].to_numpy(np.float32, copy=False)\n",
    "    Y = df[y_cols].to_numpy(np.float32, copy=False)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return X, Y, feat_cols, y_cols\n",
    "\n",
    "# ============================================================\n",
    "# 4) METRICS\n",
    "# ============================================================\n",
    "def mae(yhat, y):\n",
    "    return float(np.mean(np.abs(np.asarray(yhat, np.float32) - np.asarray(y, np.float32))))\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    d = np.asarray(yhat, np.float32) - np.asarray(y, np.float32)\n",
    "    return float(np.sqrt(np.mean(d * d)))\n",
    "\n",
    "# ============================================================\n",
    "# 5) TRAIN (multi-output, one fit per loc_id/target)\n",
    "# ============================================================\n",
    "def train_ridge_multi(loc_id: str, target_key: str, h_start: int, h_end: int):\n",
    "    name = loc_short_name(loc_id)\n",
    "    print(f\"\\n=== RIDGE MULTI-OUTPUT | {name} ({loc_id[:8]}...) target={target_key} h={h_start:03d}-{h_end:03d} ===\")\n",
    "\n",
    "    Xtr, Ytr, feat_cols, y_cols = load_XY_all(loc_id, SPLITS[\"train\"], target_key)\n",
    "    Xva, Yva, _, _ = load_XY_all(loc_id, SPLITS[\"val\"], target_key)\n",
    "    Xte, Yte, _, _ = load_XY_all(loc_id, SPLITS[\"test\"], target_key)\n",
    "\n",
    "    sl = slice(h_start - 1, h_end)\n",
    "    Ytr_s = Ytr[:, sl]\n",
    "    Yva_s = Yva[:, sl]\n",
    "    Yte_s = Yte[:, sl]\n",
    "\n",
    "    # === LOG1P TRANSFORM FOR RAIN (precipitation is heavily skewed) ===\n",
    "    use_log1p = (target_key == \"rain\")\n",
    "    if use_log1p:\n",
    "        Ytr_s = np.log1p(Ytr_s)  # Train on log1p(precip)\n",
    "        # Keep Yva_s, Yte_s in original scale for fair metric comparison\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True, copy=False)\n",
    "    scaler.fit(Xtr)\n",
    "    Xtr = scaler.transform(Xtr)\n",
    "    Xva = scaler.transform(Xva)\n",
    "    Xte = scaler.transform(Xte)\n",
    "\n",
    "    mdir = MODEL_DIR / target_key\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    mp = mdir / f\"ridge_{target_key}_{loc_id}_multi_h{h_start:03d}-{h_end:03d}.joblib\"\n",
    "    sp = mdir / f\"scaler_{target_key}_{loc_id}_lag{LAG}_h{H}.joblib\"\n",
    "    report_path = REPORT_DIR / f\"report_ridge_{target_key}_{loc_id}_h{h_start:03d}-{h_end:03d}.csv\"\n",
    "\n",
    "    if mp.exists() and sp.exists() and report_path.exists():\n",
    "        print(\"[resume] model+scaler+report exist, skip.\")\n",
    "        return pd.read_csv(report_path)\n",
    "\n",
    "    model = Ridge(\n",
    "        alpha=ALPHA,\n",
    "        fit_intercept=FIT_INTERCEPT,\n",
    "        solver=RIDGE_SOLVER,\n",
    "        tol=RIDGE_TOL,\n",
    "    )\n",
    "    model.fit(Xtr, Ytr_s)\n",
    "\n",
    "    pred_va = model.predict(Xva)\n",
    "    pred_te = model.predict(Xte)\n",
    "\n",
    "    # === INVERSE TRANSFORM FOR RAIN ===\n",
    "    if use_log1p:\n",
    "        pred_va = np.expm1(pred_va)  # expm1 = exp(x) - 1, inverse of log1p\n",
    "        pred_te = np.expm1(pred_te)\n",
    "\n",
    "    # Clip non-negative for rain (after inverse transform)\n",
    "    if target_key == \"rain\" and CLIP_NONNEG_FOR_RAIN:\n",
    "        pred_va = np.maximum(pred_va, 0.0)\n",
    "        pred_te = np.maximum(pred_te, 0.0)\n",
    "\n",
    "    rows = []\n",
    "    for i, h in enumerate(range(h_start, h_end + 1)):\n",
    "        rows.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": target_key,\n",
    "            \"h\": h,\n",
    "            \"alpha\": ALPHA,\n",
    "            \"val_mae\": mae(pred_va[:, i], Yva_s[:, i]),\n",
    "            \"val_rmse\": rmse(pred_va[:, i], Yva_s[:, i]),\n",
    "            \"test_mae\": mae(pred_te[:, i], Yte_s[:, i]),\n",
    "            \"test_rmse\": rmse(pred_te[:, i], Yte_s[:, i]),\n",
    "            \"model\": mp.name,\n",
    "        })\n",
    "\n",
    "    rep = pd.DataFrame(rows)\n",
    "    rep.to_csv(report_path, index=False)\n",
    "\n",
    "    joblib.dump(model, mp, compress=3)\n",
    "    joblib.dump(scaler, sp, compress=3)\n",
    "    print(f\"[saved] {mp.name}\")\n",
    "\n",
    "    del Xtr, Xva, Xte, Ytr, Yva, Yte, Ytr_s, Yva_s, Yte_s, model, scaler, pred_va, pred_te\n",
    "    gc.collect()\n",
    "    return rep\n",
    "\n",
    "# ============================================================\n",
    "# 6) RUN - Train theo LOCATION_ID\n",
    "# ============================================================\n",
    "summ = []\n",
    "for target in TARGETS:\n",
    "    for loc_id in LOCATION_IDS:\n",
    "        name = loc_short_name(loc_id)\n",
    "        rep = train_ridge_multi(loc_id, target, H_START, H_END)\n",
    "        summ.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": target,\n",
    "            \"n_rows\": int(len(rep)),\n",
    "            \"val_mae_mean\": float(rep[\"val_mae\"].mean()),\n",
    "            \"test_mae_mean\": float(rep[\"test_mae\"].mean()),\n",
    "            \"test_rmse_mean\": float(rep[\"test_rmse\"].mean()),\n",
    "        })\n",
    "\n",
    "leader = pd.DataFrame(summ).sort_values([\"test_mae_mean\"], ascending=True)\n",
    "leader_path = REPORT_DIR / \"ridge_leaderboard.csv\"\n",
    "leader.to_csv(leader_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Saved leaderboard:\", leader_path)\n",
    "leader"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
