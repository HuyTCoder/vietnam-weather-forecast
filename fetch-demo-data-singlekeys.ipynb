{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Kaggle-ready: Fetch + Clean (no leakage) + Build Tabular/Seq\n",
    "# SINGLE KEY SET (no duplicate target keys)\n",
    "# - Canonical keys: temp, rain, u10, v10, rh, press, cloud\n",
    "# - Files named by LOCATION_ID (UUID) for consistency\n",
    "# - 20 PROVINCES/CITIES OF VIETNAM (optimized for Kaggle free tier)\n",
    "# ============================================\n",
    "\n",
    "!pip -q install -U pyarrow requests\n",
    "\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "from uuid import uuid5, NAMESPACE_DNS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# -------------------------\n",
    "# 1) Paths / settings (optimized for Kaggle)\n",
    "# -------------------------\n",
    "BASE_DIR = Path(\"/kaggle/working/weather_20loc\")\n",
    "RAW_DIR  = BASE_DIR / \"data\" / \"raw\"\n",
    "PROC_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "TAB_DIR  = BASE_DIR / \"data\" / \"tabular\"\n",
    "SEQ_DIR  = BASE_DIR / \"data\" / \"sequences\"\n",
    "META_DIR = BASE_DIR / \"data\" / \"meta\"\n",
    "for p in [RAW_DIR, PROC_DIR, TAB_DIR, SEQ_DIR, META_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# 2) 20 PROVINCES/CITIES OF VIETNAM (đại diện đủ các vùng miền)\n",
    "# -------------------------\n",
    "LOCATIONS = [\n",
    "    # ===== MIỀN BẮC (7 tỉnh) =====\n",
    "    {\"name\": \"hanoi\",        \"lat\": 21.0285, \"lon\": 105.8542},  # 1. TP Hà Nội\n",
    "    {\"name\": \"haiphong\",     \"lat\": 20.8449, \"lon\": 106.6881},  # 2. TP Hải Phòng\n",
    "    {\"name\": \"quangninh\",    \"lat\": 21.0064, \"lon\": 107.2925},  # 3. Quảng Ninh\n",
    "    {\"name\": \"laocai\",       \"lat\": 22.4856, \"lon\": 103.9707},  # 4. Lào Cai\n",
    "    {\"name\": \"hagiang\",      \"lat\": 22.8233, \"lon\": 104.9833},  # 5. Hà Giang\n",
    "    {\"name\": \"thainguyen\",   \"lat\": 21.5928, \"lon\": 105.8442},  # 6. Thái Nguyên\n",
    "    {\"name\": \"thanhhoa\",     \"lat\": 19.8066, \"lon\": 105.7852},  # 7. Thanh Hóa\n",
    "    \n",
    "    # ===== MIỀN TRUNG (7 tỉnh) =====\n",
    "    {\"name\": \"nghean\",       \"lat\": 19.2342, \"lon\": 104.9200},  # 8. Nghệ An\n",
    "    {\"name\": \"hatinh\",       \"lat\": 18.3559, \"lon\": 105.8877},  # 9. Hà Tĩnh\n",
    "    {\"name\": \"hue\",          \"lat\": 16.4674, \"lon\": 107.5905},  # 10. Thừa Thiên Huế\n",
    "    {\"name\": \"danang\",       \"lat\": 16.0544, \"lon\": 108.2022},  # 11. TP Đà Nẵng\n",
    "    {\"name\": \"quangnam\",     \"lat\": 15.5735, \"lon\": 108.4740},  # 12. Quảng Nam\n",
    "    {\"name\": \"khanhhoa\",     \"lat\": 12.2585, \"lon\": 109.0526},  # 13. Khánh Hòa\n",
    "    {\"name\": \"ninhthuan\",    \"lat\": 11.5752, \"lon\": 108.9829},  # 14. Ninh Thuận\n",
    "    \n",
    "    # ===== TÂY NGUYÊN + NAM BỘ (6 tỉnh) =====\n",
    "    {\"name\": \"gialai\",       \"lat\": 13.9832, \"lon\": 108.0025},  # 15. Gia Lai\n",
    "    {\"name\": \"daklak\",       \"lat\": 12.6800, \"lon\": 108.0378},  # 16. Đắk Lắk\n",
    "    {\"name\": \"lamdong\",      \"lat\": 11.9404, \"lon\": 108.4583},  # 17. Lâm Đồng\n",
    "    {\"name\": \"hcmc\",         \"lat\": 10.8231, \"lon\": 106.6297},  # 18. TP Hồ Chí Minh\n",
    "    {\"name\": \"dongnai\",      \"lat\": 10.9453, \"lon\": 106.8243},  # 19. Đồng Nai\n",
    "    {\"name\": \"cantho\",       \"lat\": 10.0452, \"lon\": 105.7469},  # 20. TP Cần Thơ\n",
    "]\n",
    "\n",
    "def make_ids(name, lat, lon):\n",
    "    grid_id = f\"{lat:.4f}_{lon:.4f}\"\n",
    "    location_id = str(uuid5(NAMESPACE_DNS, f\"loc::{name}::{grid_id}\"))\n",
    "    return location_id, grid_id\n",
    "\n",
    "for loc in LOCATIONS:\n",
    "    loc[\"location_id\"], loc[\"grid_id\"] = make_ids(loc[\"name\"], loc[\"lat\"], loc[\"lon\"])\n",
    "\n",
    "# Tạo mapping và lưu ra file để training notebooks sử dụng\n",
    "LOCATION_IDS = [loc[\"location_id\"] for loc in LOCATIONS]\n",
    "LOC_META = {loc[\"location_id\"]: {\"name\": loc[\"name\"], \"lat\": loc[\"lat\"], \"lon\": loc[\"lon\"], \"grid_id\": loc[\"grid_id\"]} for loc in LOCATIONS}\n",
    "\n",
    "# Lưu metadata\n",
    "meta_path = META_DIR / \"locations.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump({\"locations\": LOCATIONS, \"location_ids\": LOCATION_IDS}, f, indent=2)\n",
    "print(f\"[ok] Saved location metadata: {meta_path}\")\n",
    "print(f\"[info] Total locations: {len(LOCATIONS)}\")\n",
    "\n",
    "for loc in LOCATIONS:\n",
    "    print(f\"  {loc['name']:15s} -> {loc['location_id']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Time ranges / splits (optimized: 3 năm train, 1 năm val, 11 tháng test)\n",
    "# -------------------------\n",
    "FULL_START = \"2021-01-01\"\n",
    "FULL_END   = \"2025-11-30\"\n",
    "\n",
    "SPLITS = {\n",
    "    \"train_2021_2023\": (\"2021-01-01\", \"2023-12-31\"),\n",
    "    \"val_2024\":        (\"2024-01-01\", \"2024-12-31\"),\n",
    "    \"test_2025_01_to_2025_11\": (\"2025-01-01\", \"2025-11-30\"),\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 4) HYPERPARAMETERS\n",
    "# -------------------------\n",
    "LAG = 49            # 49h lookback\n",
    "HORIZON = 100       # 100h forecast (~4 days)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Open-Meteo archive fetch (với rate limit handling tốt hơn)\n",
    "# -------------------------\n",
    "ARCHIVE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "HOURLY_VARS = [\n",
    "    \"temperature_2m\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"precipitation\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"surface_pressure\",\n",
    "    \"cloud_cover\",\n",
    "]\n",
    "\n",
    "def _safe_get(url, params, tries=5, backoff=2.0, timeout=60):\n",
    "    \"\"\"Safe GET with exponential backoff, đặc biệt cho 429 errors\"\"\"\n",
    "    last = None\n",
    "    for t in range(tries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout)\n",
    "            if r.status_code == 429:\n",
    "                # Rate limited - wait longer\n",
    "                wait_time = backoff ** (t + 2)  # Longer wait for rate limit\n",
    "                print(f\"[rate limit] waiting {wait_time:.1f}s...\", end=\" \", flush=True)\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            wait_time = backoff ** t\n",
    "            time.sleep(wait_time)\n",
    "    raise RuntimeError(f\"Request failed after {tries} tries: {last}\")\n",
    "\n",
    "def fetch_openmeteo_hourly_archive(loc, start_date, end_date):\n",
    "    params = {\n",
    "        \"latitude\": loc[\"lat\"],\n",
    "        \"longitude\": loc[\"lon\"],\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \",\".join(HOURLY_VARS),\n",
    "        \"timezone\": \"UTC\",\n",
    "        \"wind_speed_unit\": \"ms\",\n",
    "        \"precipitation_unit\": \"mm\",\n",
    "        \"timeformat\": \"iso8601\",\n",
    "    }\n",
    "    r = _safe_get(ARCHIVE_URL, params=params)\n",
    "    js = r.json()\n",
    "    hourly = js.get(\"hourly\", {})\n",
    "    out = pd.DataFrame({\"time\": hourly.get(\"time\", [])})\n",
    "    for v in HOURLY_VARS:\n",
    "        out[v] = hourly.get(v, None)\n",
    "    out[\"location_id\"] = loc[\"location_id\"]\n",
    "    out[\"grid_id\"] = loc[\"grid_id\"]\n",
    "    return out\n",
    "\n",
    "def fetch_full_to_raw():\n",
    "    total = len(LOCATIONS)\n",
    "    for i, loc in enumerate(LOCATIONS, 1):\n",
    "        # File đặt tên theo location_id\n",
    "        raw_path = RAW_DIR / f\"{loc['location_id']}_hourly_raw.parquet\"\n",
    "        if raw_path.exists():\n",
    "            print(f\"[skip {i}/{total}] raw exists:\", raw_path.name)\n",
    "            continue\n",
    "        print(f\"[fetch {i}/{total}] {loc['name']}...\", end=\" \", flush=True)\n",
    "        df = fetch_openmeteo_hourly_archive(loc, FULL_START, FULL_END)\n",
    "        df.to_parquet(raw_path, index=False)\n",
    "        print(f\"-> {raw_path.name}\", df.shape)\n",
    "        # Delay để tránh rate limit\n",
    "        time.sleep(1.5)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Clean (NO leakage)\n",
    "# -------------------------\n",
    "def wind_to_uv(speed_ms, dir_deg):\n",
    "    sp = pd.to_numeric(speed_ms, errors=\"coerce\").astype(\"float32\")\n",
    "    dd = pd.to_numeric(dir_deg, errors=\"coerce\").astype(\"float32\")\n",
    "    rad = np.deg2rad(dd)\n",
    "    u = -sp * np.sin(rad)\n",
    "    v = -sp * np.cos(rad)\n",
    "    return u.astype(\"float32\"), v.astype(\"float32\")\n",
    "\n",
    "def uv_to_wind_dir(u, v):\n",
    "    u = pd.to_numeric(u, errors=\"coerce\").astype(\"float32\")\n",
    "    v = pd.to_numeric(v, errors=\"coerce\").astype(\"float32\")\n",
    "    deg = (np.degrees(np.arctan2(-u, -v)) + 360.0) % 360.0\n",
    "    return deg.astype(\"float32\")\n",
    "\n",
    "def add_time_feats(df, time_col=\"time\"):\n",
    "    t = pd.to_datetime(df[time_col], utc=True)\n",
    "    hour = t.dt.hour.astype(np.float32)\n",
    "    doy  = t.dt.dayofyear.astype(np.float32)\n",
    "    df[\"hour_sin\"] = np.sin(2*np.pi*hour/24.0).astype(\"float32\")\n",
    "    df[\"hour_cos\"] = np.cos(2*np.pi*hour/24.0).astype(\"float32\")\n",
    "    df[\"doy_sin\"]  = np.sin(2*np.pi*doy/365.25).astype(\"float32\")\n",
    "    df[\"doy_cos\"]  = np.cos(2*np.pi*doy/365.25).astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def clean_hourly_df(df, time_col=\"time\", freq=\"1h\", max_interp_gap_hours=6, max_ffill_gap_hours=6, clip_precip_nonneg=True):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    loc_id = df[\"location_id\"].iloc[0] if \"location_id\" in df.columns and len(df) else None\n",
    "    grid_id = df[\"grid_id\"].iloc[0] if \"grid_id\" in df.columns and len(df) else None\n",
    "\n",
    "    for c in HOURLY_VARS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    df[\"u10\"], df[\"v10\"] = wind_to_uv(df[\"wind_speed_10m\"], df[\"wind_direction_10m\"])\n",
    "    df = df.set_index(time_col)\n",
    "\n",
    "    start = df.index.min().floor(freq)\n",
    "    end   = df.index.max().ceil(freq)\n",
    "    full_index = pd.date_range(start=start, end=end, freq=freq, tz=\"UTC\")\n",
    "    df = df.reindex(full_index)\n",
    "\n",
    "    lim = int(max_interp_gap_hours)\n",
    "    ffill_lim = int(max_ffill_gap_hours)\n",
    "\n",
    "    # Continuous cols: interpolate + ffill for small gaps\n",
    "    cont_cols = [\"temperature_2m\",\"relative_humidity_2m\",\"u10\",\"v10\",\"surface_pressure\",\"cloud_cover\"]\n",
    "    for c in cont_cols:\n",
    "        s = df[c]\n",
    "        s = s.interpolate(method=\"time\", limit=lim, limit_direction=\"forward\")\n",
    "        s = s.ffill(limit=ffill_lim)\n",
    "        df[c] = s.astype(\"float32\")\n",
    "\n",
    "    # Precipitation: fill 0 for small gaps\n",
    "    precip = df[\"precipitation\"]\n",
    "    precip = precip.ffill(limit=ffill_lim)\n",
    "    precip = precip.fillna(0.0)\n",
    "    df[\"precipitation\"] = precip.astype(\"float32\")\n",
    "\n",
    "    df[\"wind_speed_10m\"] = np.sqrt(df[\"u10\"]**2 + df[\"v10\"]**2).astype(\"float32\")\n",
    "    df[\"wind_direction_10m\"] = uv_to_wind_dir(df[\"u10\"], df[\"v10\"]).astype(\"float32\")\n",
    "\n",
    "    if clip_precip_nonneg:\n",
    "        df[\"precipitation\"] = np.maximum(df[\"precipitation\"].to_numpy(), 0.0).astype(\"float32\")\n",
    "\n",
    "    df = df.reset_index().rename(columns={\"index\": time_col})\n",
    "    df = add_time_feats(df, time_col=time_col)\n",
    "\n",
    "    if loc_id is not None:\n",
    "        df[\"location_id\"] = loc_id\n",
    "    if grid_id is not None:\n",
    "        df[\"grid_id\"] = grid_id\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_full_raw_to_processed():\n",
    "    total = len(LOCATIONS)\n",
    "    for i, loc in enumerate(LOCATIONS, 1):\n",
    "        raw_path  = RAW_DIR  / f\"{loc['location_id']}_hourly_raw.parquet\"\n",
    "        proc_path = PROC_DIR / f\"{loc['location_id']}_hourly_proc.parquet\"\n",
    "        if proc_path.exists():\n",
    "            print(f\"[skip {i}/{total}] proc exists:\", proc_path.name)\n",
    "            continue\n",
    "        df = pd.read_parquet(raw_path)\n",
    "        if \"location_id\" not in df.columns:\n",
    "            df[\"location_id\"] = loc[\"location_id\"]\n",
    "        if \"grid_id\" not in df.columns:\n",
    "            df[\"grid_id\"] = loc[\"grid_id\"]\n",
    "        df = clean_hourly_df(df)\n",
    "        df.to_parquet(proc_path, index=False)\n",
    "        print(f\"[ok {i}/{total}] processed {loc['name']} -> {proc_path.name}\", df.shape)\n",
    "        gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# 7) Build tabular lagged + sequences (OPTIMIZED)\n",
    "# -------------------------\n",
    "FEATURE_COLS = [\n",
    "    \"temperature_2m\",\"relative_humidity_2m\",\"precipitation\",\n",
    "    \"u10\",\"v10\",\"wind_speed_10m\",\n",
    "    \"surface_pressure\",\"cloud_cover\",\n",
    "    \"hour_sin\",\"hour_cos\",\"doy_sin\",\"doy_cos\",\n",
    "]\n",
    "\n",
    "TAB_TARGETS = {\n",
    "    \"temp\":  \"temperature_2m\",\n",
    "    \"rain\":  \"precipitation\",\n",
    "    \"u10\":   \"u10\",\n",
    "    \"v10\":   \"v10\",\n",
    "    \"rh\":    \"relative_humidity_2m\",\n",
    "    \"press\": \"surface_pressure\",\n",
    "    \"cloud\": \"cloud_cover\",\n",
    "}\n",
    "\n",
    "GRU_INPUT_COLS  = FEATURE_COLS\n",
    "GRU_TARGET_COLS = [\n",
    "    \"temperature_2m\",\"precipitation\",\"u10\",\"v10\",\n",
    "    \"relative_humidity_2m\",\"surface_pressure\",\"cloud_cover\"\n",
    "]\n",
    "\n",
    "def build_X_lags(d, time_col=\"time\"):\n",
    "    \"\"\"Build lagged features efficiently using numpy\"\"\"\n",
    "    data = d[FEATURE_COLS].to_numpy(dtype=np.float32)\n",
    "    n_samples = len(d)\n",
    "    n_features = len(FEATURE_COLS)\n",
    "    \n",
    "    # Pre-allocate array for lagged features\n",
    "    lag_data = np.empty((n_samples, LAG * n_features), dtype=np.float32)\n",
    "    lag_cols = []\n",
    "    \n",
    "    for l in range(LAG):\n",
    "        start_col = l * n_features\n",
    "        end_col = start_col + n_features\n",
    "        if l == 0:\n",
    "            lag_data[:, start_col:end_col] = data\n",
    "        else:\n",
    "            lag_data[l:, start_col:end_col] = data[:-l]\n",
    "            lag_data[:l, start_col:end_col] = np.nan\n",
    "        \n",
    "        lag_cols.extend([f\"{c}_lag{l:02d}\" for c in FEATURE_COLS])\n",
    "    \n",
    "    X_lags = pd.DataFrame(lag_data, columns=lag_cols)\n",
    "    \n",
    "    keep_meta = [time_col]\n",
    "    for meta in [\"location_id\",\"grid_id\"]:\n",
    "        if meta in d.columns:\n",
    "            keep_meta.append(meta)\n",
    "    return d[keep_meta].reset_index(drop=True), X_lags\n",
    "\n",
    "def make_tabular_lagged_from_precomputed(meta_df, X_lags, d, target_col):\n",
    "    \"\"\"Build tabular data with future targets efficiently\"\"\"\n",
    "    data = d[target_col].to_numpy(dtype=np.float32)\n",
    "    n_samples = len(d)\n",
    "    \n",
    "    # Pre-allocate array for future targets\n",
    "    y_data = np.empty((n_samples, HORIZON), dtype=np.float32)\n",
    "    y_cols = []\n",
    "    \n",
    "    for h in range(1, HORIZON + 1):\n",
    "        if h < n_samples:\n",
    "            y_data[:-h, h-1] = data[h:]\n",
    "            y_data[-h:, h-1] = np.nan\n",
    "        else:\n",
    "            y_data[:, h-1] = np.nan\n",
    "        y_cols.append(f\"y_t+{h:03d}\")\n",
    "    \n",
    "    Y = pd.DataFrame(y_data, columns=y_cols)\n",
    "    out = pd.concat([meta_df.reset_index(drop=True), X_lags.reset_index(drop=True), Y], axis=1)\n",
    "    out = out.dropna().reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def make_rnn_sequences(df, time_col=\"time\"):\n",
    "    \"\"\"Build sequences efficiently using sliding window view\"\"\"\n",
    "    d = df.copy()\n",
    "    d[time_col] = pd.to_datetime(d[time_col], utc=True, errors=\"coerce\")\n",
    "    d = d.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    X = d[GRU_INPUT_COLS].to_numpy(np.float32)\n",
    "    Y = d[GRU_TARGET_COLS].to_numpy(np.float32)\n",
    "    t = d[time_col].to_numpy(\"datetime64[ns]\")\n",
    "    loc_id = d[\"location_id\"].iloc[0] if \"location_id\" in d.columns else \"\"\n",
    "\n",
    "    n = len(d)\n",
    "    n_samples = n - LAG - HORIZON + 1\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(f\"Not enough rows: n={n}, need at least {LAG+HORIZON}\")\n",
    "\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "    Xw = sliding_window_view(X, window_shape=(LAG, X.shape[1]))\n",
    "    Xw = Xw[:, 0, :, :]\n",
    "    Xs = Xw[:n_samples].astype(np.float32)\n",
    "\n",
    "    Yw = sliding_window_view(Y[LAG:], window_shape=(HORIZON, Y.shape[1]))\n",
    "    Yw = Yw[:, 0, :, :]\n",
    "    Ys = Yw[:n_samples].astype(np.float32)\n",
    "\n",
    "    Ts = t[LAG-1 : LAG-1 + n_samples]\n",
    "    meta = {\"lag\": LAG, \"horizon\": HORIZON, \"x_cols\": GRU_INPUT_COLS, \"y_cols\": GRU_TARGET_COLS, \"location_id\": loc_id}\n",
    "    return Xs, Ys, Ts, meta\n",
    "\n",
    "def build_all_outputs():\n",
    "    \"\"\"Build tabular and sequence outputs with memory optimization\"\"\"\n",
    "    total = len(LOCATIONS)\n",
    "    for i, loc in enumerate(LOCATIONS, 1):\n",
    "        loc_id = loc[\"location_id\"]\n",
    "        proc_path = PROC_DIR / f\"{loc_id}_hourly_proc.parquet\"\n",
    "        \n",
    "        # Check if all outputs already exist\n",
    "        all_exist = True\n",
    "        for key in TAB_TARGETS:\n",
    "            for split_name in SPLITS:\n",
    "                tab_path = TAB_DIR / f\"{loc_id}_{split_name}_tab_{key}_lag{LAG}_h{HORIZON}.parquet\"\n",
    "                if not tab_path.exists():\n",
    "                    all_exist = False\n",
    "                    break\n",
    "            if not all_exist:\n",
    "                break\n",
    "        for split_name in SPLITS:\n",
    "            seq_path = SEQ_DIR / f\"{loc_id}_{split_name}_seq_multi_lag{LAG}_h{HORIZON}.npz\"\n",
    "            if not seq_path.exists():\n",
    "                all_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_exist:\n",
    "            print(f\"[skip {i}/{total}] {loc['name']} outputs exist\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"[build {i}/{total}] {loc['name']}...\")\n",
    "        \n",
    "        df = pd.read_parquet(proc_path)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True)\n",
    "\n",
    "        d = df.sort_values(\"time\").reset_index(drop=True)\n",
    "        meta_df, X_lags = build_X_lags(d, time_col=\"time\")\n",
    "\n",
    "        # Tabular files - process one target at a time to save memory\n",
    "        for key, col in TAB_TARGETS.items():\n",
    "            tab_all = make_tabular_lagged_from_precomputed(meta_df, X_lags, d, target_col=col)\n",
    "            tab_all[\"time\"] = pd.to_datetime(tab_all[\"time\"], utc=True)\n",
    "\n",
    "            for split_name, (s, e) in SPLITS.items():\n",
    "                s = pd.to_datetime(s, utc=True)\n",
    "                e = pd.to_datetime(e, utc=True)\n",
    "                issue_end = e - pd.Timedelta(hours=HORIZON)\n",
    "                mask = (tab_all[\"time\"] >= s) & (tab_all[\"time\"] <= issue_end)\n",
    "                out = tab_all.loc[mask].reset_index(drop=True)\n",
    "\n",
    "                out_path = TAB_DIR / f\"{loc_id}_{split_name}_tab_{key}_lag{LAG}_h{HORIZON}.parquet\"\n",
    "                out.to_parquet(out_path, index=False, compression='snappy')\n",
    "                print(f\"  [ok] tabular {loc['name']}/{key}/{split_name}\", out.shape)\n",
    "\n",
    "                del out\n",
    "                gc.collect()\n",
    "\n",
    "            del tab_all\n",
    "            gc.collect()\n",
    "\n",
    "        # Sequences\n",
    "        Xs, Ys, Ts, meta = make_rnn_sequences(d)\n",
    "        Ts = pd.to_datetime(Ts, utc=True)\n",
    "\n",
    "        for split_name, (s, e) in SPLITS.items():\n",
    "            s = pd.to_datetime(s, utc=True)\n",
    "            e = pd.to_datetime(e, utc=True)\n",
    "            issue_end = e - pd.Timedelta(hours=HORIZON)\n",
    "\n",
    "            idx = (Ts >= s) & (Ts <= issue_end)\n",
    "            X_out, Y_out, T_out = Xs[idx], Ys[idx], Ts[idx].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "            seq_path = SEQ_DIR / f\"{loc_id}_{split_name}_seq_multi_lag{LAG}_h{HORIZON}.npz\"\n",
    "            np.savez_compressed(seq_path, X=X_out, Y=Y_out, T=T_out, meta=json.dumps(meta))\n",
    "            print(f\"  [ok] sequences {loc['name']}/{split_name}\", X_out.shape, Y_out.shape)\n",
    "\n",
    "        del df, d, meta_df, X_lags, Xs, Ys, Ts\n",
    "        gc.collect()\n",
    "\n",
    "# -------------------------\n",
    "# RUN\n",
    "# -------------------------\n",
    "fetch_full_to_raw()\n",
    "process_full_raw_to_processed()\n",
    "build_all_outputs()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total locations: {len(LOCATIONS)}\")\n",
    "print(f\"Tabular dir: {TAB_DIR}\")\n",
    "print(f\"Sequences dir: {SEQ_DIR}\")\n",
    "print(f\"Location metadata: {meta_path}\")\n",
    "print(f\"LAG (lookback): {LAG}h\")\n",
    "print(f\"HORIZON (forecast): {HORIZON}h ({HORIZON//24} days)\")\n",
    "print(\"\\nLocation IDs:\")\n",
    "for loc in LOCATIONS:\n",
    "    print(f\"  {loc['name']:15s} = {loc['location_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b171834",
   "metadata": {},
   "source": [
    "## ✅ Sau khi chạy xong: Upload thành Kaggle Dataset\n",
    "\n",
    "**Output structure (20 tỉnh/thành):**\n",
    "```\n",
    "/kaggle/working/weather_20loc/data/\n",
    "├── meta/\n",
    "│   └── locations.json       <- mapping location_id -> name, lat, lon (20 locations)\n",
    "├── tabular/\n",
    "│   └── {location_id}_{split}_tab_{target}_lag49_h100.parquet\n",
    "│       (20 locations × 7 targets × 3 splits = 420 files)\n",
    "└── sequences/\n",
    "    └── {location_id}_{split}_seq_multi_lag49_h100.npz\n",
    "        (20 locations × 3 splits = 60 files)\n",
    "```\n",
    "\n",
    "**Danh sách 34 tỉnh/thành (theo quy hoạch sáp nhập):**\n",
    "- **Miền Bắc (15):** Hà Nội, Quảng Ninh, Cao Bằng, Lạng Sơn, Lai Châu, Điện Biên, Sơn La, Tuyên Quang (+HG), Lào Cai (+YB), Thái Nguyên (+BK), Phú Thọ (+HB,VP), Bắc Ninh (+BG), Hưng Yên (+TB), Hải Phòng (+HD), Ninh Bình (+HN,NĐ)\n",
    "- **Miền Trung (11):** Thanh Hóa, Nghệ An, Hà Tĩnh, Quảng Trị (+QB), Huế, Đà Nẵng (+QN), Quảng Ngãi (+KT), Gia Lai (+BĐ), Khánh Hòa (+NT), Đắk Lắk (+PY), Lâm Đồng (+ĐN,BT)\n",
    "- **Miền Nam (8):** TPHCM (+BV,BD), Đồng Nai (+BP), Tây Ninh (+LA), Cần Thơ (+ST,HG), Vĩnh Long (+BT,TV), Đồng Tháp (+TG), Cà Mau (+BL), An Giang (+KG)\n",
    "\n",
    "**Thay đổi so với phiên bản 63 tỉnh:**\n",
    "- Giảm từ 63 xuống 20 locations để tránh rate limit API\n",
    "- Tăng delay giữa requests lên 2 giây\n",
    "- Cải thiện xử lý 429 errors với exponential backoff\n",
    "\n",
    "**Bước tiếp theo:**\n",
    "1. Chạy notebook trên Kaggle (~15-20 phút để fetch)\n",
    "2. Download folder `weather_20loc/` từ Output\n",
    "3. Upload lên Kaggle Dataset\n",
    "4. Add dataset vào training notebooks (GRU, TCN, XGB, LGB, Ridge)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
