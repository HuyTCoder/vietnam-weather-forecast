{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b63dfa0",
   "metadata": {},
   "source": [
    "## GRU (PyTorch) — sequences from fetch singlekeys (20 Locations)\n",
    "\n",
    "**Yêu cầu Dataset:**\n",
    "- Chạy notebook `fetch-demo-data-singlekeys.ipynb` trước (đã fetch 20 tỉnh/thành)\n",
    "- Upload output thành Kaggle Dataset\n",
    "- Add dataset vào notebook này\n",
    "\n",
    "**Dữ liệu format:**\n",
    "- Files theo **location_id** (UUID), không theo tên location\n",
    "- Ví dụ: `{location_id}_train_2021_2023_seq_multi_lag49_h100.npz`\n",
    "- Metadata: `weather_20loc/data/meta/locations.json` chứa mapping location_id -> name\n",
    "\n",
    "**Config:**\n",
    "- LAG = 49h lookback\n",
    "- HORIZON = 100h forecast (~4 ngày)  \n",
    "- 20 locations thay vì 34/63\n",
    "- Batch size = 96, Hidden = 192 (tối ưu cho T4 GPU)\n",
    "- AMP (Mixed Precision) để tăng tốc\n",
    "\n",
    "**Features:**\n",
    "- Accelerator: gpu / cpu\n",
    "- Auto-detect paths: tự dò SEQ_DIR trong `/kaggle/input/<dataset>/weather_20loc/data/sequences`\n",
    "- Report: bao gồm location_id column trong CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# train_gru_weather_v3_full.py\n",
    "# GRU multi-target weather + precip 2-step (event+amount)\n",
    "# v3: streaming (no concat), true log1p amount, pos_weight BCE, clean AMP API\n",
    "# v3.1: location_id tracking from metadata\n",
    "# 20 locations, LAG=49, HORIZON=100\n",
    "# ============================================================\n",
    "\n",
    "import os, json, gc, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) ONE-LINE SWITCH\n",
    "# ============================================================\n",
    "ACCELERATOR = \"gpu\"   # \"gpu\" | \"cpu\"\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# 1) CONFIG (OPTIMIZED for 20 locations + Kaggle Free Tier)\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # === DATA PATHS (auto-detected) ===\n",
    "    base_dir: str = \"/kaggle/input/general-demo-data/weather_20loc\"\n",
    "    seq_dir_rel: str = \"data/sequences\"\n",
    "    meta_dir_rel: str = \"data/meta\"\n",
    "\n",
    "    lag: int = 49           # 49h lookback\n",
    "    horizon: int = 100      # 100h forecast (~4 days)\n",
    "\n",
    "    # === LOCATION BATCHING ===\n",
    "    start_loc_idx: int = 0\n",
    "    end_loc_idx: int = -1   # -1 = all remaining\n",
    "\n",
    "    # model (OPTIMIZED: smaller for Kaggle)\n",
    "    hidden: int = 192       # Reduced from 256\n",
    "    layers: int = 2\n",
    "    dropout: float = 0.15\n",
    "\n",
    "    # train (OPTIMIZED)\n",
    "    epochs: int = 20        # Reduced from 25\n",
    "    batch_size: int = 96    # Optimized for T4 GPU\n",
    "    lr: float = 2e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    patience: int = 5\n",
    "\n",
    "    # precip\n",
    "    rain_thr_mm: float = 0.1\n",
    "    amount_space: str = \"log1p\"\n",
    "    precip_mm_max: float = 200.0\n",
    "    loss_cls_w: float = 0.35\n",
    "    loss_reg_w: float = 0.65\n",
    "    pos_weight_cap: float = 20.0\n",
    "\n",
    "    # weights for other targets\n",
    "    w_temp: float = 1.0\n",
    "    w_rh: float = 0.5\n",
    "    w_press: float = 0.2\n",
    "    w_cloud: float = 0.5\n",
    "    w_wind: float = 0.7\n",
    "\n",
    "    # TPU (disabled)\n",
    "    tpu_cores: int = 8\n",
    "\n",
    "    # report\n",
    "    run_report: bool = True\n",
    "    p_thr_cand: Tuple[float, ...] = tuple(np.round(np.linspace(0.05, 0.95, 19), 2).tolist())\n",
    "    bins: Tuple[Tuple[int, int], ...] = ((1,24),(25,48),(49,72),(73,100))\n",
    "\n",
    "    # output\n",
    "    out_dir: str = \"/kaggle/working/gru_weather_v3_out\"\n",
    "\n",
    "cfg = CFG()\n",
    "OUT_DIR = Path(cfg.out_dir)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR = OUT_DIR / \"reports\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2) HELPERS\n",
    "# ============================================================\n",
    "def is_tpu() -> bool: return ACCELERATOR.lower() == \"tpu\"\n",
    "def is_gpu() -> bool: return ACCELERATOR.lower() == \"gpu\"\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "def find_base_dir() -> Path:\n",
    "    \"\"\"Find the base directory containing data/sequences and data/meta.\"\"\"\n",
    "    p = Path(cfg.base_dir)\n",
    "    if (p / cfg.seq_dir_rel).exists():\n",
    "        return p\n",
    "    root = Path(\"/kaggle/input\")\n",
    "    # Try weather_20loc first, then others\n",
    "    for pattern in [\"weather_20loc\", \"weather_34loc\", \"weather_63loc\", \"weather_4loc\"]:\n",
    "        hits = list(root.rglob(f\"{pattern}/data/sequences\"))\n",
    "        if hits:\n",
    "            return hits[0].parent.parent\n",
    "    hits = list(root.rglob(\"data/sequences\"))\n",
    "    if hits:\n",
    "        return hits[0].parent.parent\n",
    "    hits = list(root.rglob(f\"*_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"))\n",
    "    if hits:\n",
    "        return hits[0].parent.parent.parent\n",
    "    raise FileNotFoundError(\"Cannot find base dir in /kaggle/input\")\n",
    "\n",
    "BASE_DIR = find_base_dir()\n",
    "SEQ_DIR = BASE_DIR / cfg.seq_dir_rel\n",
    "META_DIR = BASE_DIR / cfg.meta_dir_rel\n",
    "print(\"[BASE_DIR]\", BASE_DIR)\n",
    "print(\"[SEQ_DIR]\", SEQ_DIR)\n",
    "print(\"[META_DIR]\", META_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 2.1) LOCATION ID LOADING\n",
    "# ============================================================\n",
    "def load_location_ids() -> Tuple[List[str], Dict[str, str]]:\n",
    "    \"\"\"Load location_ids from metadata file, or fallback to scanning files.\"\"\"\n",
    "    meta_file = META_DIR / \"locations.json\"\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        loc_ids = meta.get(\"location_ids\", [])\n",
    "        loc_names = {loc[\"location_id\"]: loc[\"name\"] for loc in meta.get(\"locations\", [])}\n",
    "        print(f\"[meta] Loaded {len(loc_ids)} location_ids from {meta_file.name}\")\n",
    "        return loc_ids, loc_names\n",
    "    \n",
    "    # Fallback: scan files\n",
    "    pattern = f\"*_train_2021_2023_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"\n",
    "    files = sorted(SEQ_DIR.glob(pattern))\n",
    "    loc_ids = []\n",
    "    for fp in files:\n",
    "        loc_id = fp.name.split(\"_train_\")[0]\n",
    "        if loc_id and loc_id not in loc_ids:\n",
    "            loc_ids.append(loc_id)\n",
    "    print(f\"[fallback] Found {len(loc_ids)} location_ids from file scan\")\n",
    "    return loc_ids, {lid: lid[:8] for lid in loc_ids}\n",
    "\n",
    "LOCATION_IDS_ALL, LOC_NAMES = load_location_ids()\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "_start = cfg.start_loc_idx\n",
    "_end = cfg.end_loc_idx if cfg.end_loc_idx >= 0 else len(LOCATION_IDS_ALL)\n",
    "LOCATION_IDS = LOCATION_IDS_ALL[_start:_end]\n",
    "print(f\"[LOCATION BATCH] Using {len(LOCATION_IDS)}/{len(LOCATION_IDS_ALL)} locations (idx {_start}:{_end})\")\n",
    "for lid in LOCATION_IDS:\n",
    "    print(f\"  {lid} -> {LOC_NAMES.get(lid, '?')}\")\n",
    "\n",
    "def list_npz(split_key: str) -> List[Path]:\n",
    "    files = sorted(SEQ_DIR.glob(f\"*_{split_key}_seq_multi_lag{cfg.lag}_h{cfg.horizon}.npz\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No npz for split={split_key} in {SEQ_DIR}\")\n",
    "    return files\n",
    "\n",
    "def get_loc_id_from_file(fp: Path) -> str:\n",
    "    \"\"\"Extract location_id from filename.\"\"\"\n",
    "    name = fp.name\n",
    "    for split in [\"train_2021_2023\", \"val_2024\", \"test_2025_01_to_2025_11\"]:\n",
    "        if f\"_{split}_\" in name:\n",
    "            return name.split(f\"_{split}_\")[0]\n",
    "    return name.split(\"_\")[0]\n",
    "\n",
    "TRAIN_FILES = list_npz(\"train_2021_2023\")\n",
    "VAL_FILES   = list_npz(\"val_2024\")\n",
    "TEST_FILES  = list_npz(\"test_2025_01_to_2025_11\")\n",
    "print(\"[files] train:\", len(TRAIN_FILES), \"val:\", len(VAL_FILES), \"test:\", len(TEST_FILES))\n",
    "\n",
    "def load_npz(fp: Path):\n",
    "    z = np.load(fp, allow_pickle=False)\n",
    "    X = z[\"X\"].astype(np.float32, copy=False)\n",
    "    Y = z[\"Y\"].astype(np.float32, copy=False)\n",
    "    T = z[\"T\"]\n",
    "    meta = json.loads(z[\"meta\"].item())\n",
    "    return X, Y, T, meta\n",
    "\n",
    "# read meta once\n",
    "X0, Y0, T0, meta = load_npz(TRAIN_FILES[0])\n",
    "input_cols = meta.get(\"input_cols\") or meta.get(\"x_cols\")\n",
    "target_cols = meta.get(\"target_cols\") or meta.get(\"y_cols\")\n",
    "if input_cols is None or target_cols is None:\n",
    "    raise KeyError(f\"Meta missing input/target cols. Keys={list(meta.keys())}\")\n",
    "IDX = {c:i for i,c in enumerate(target_cols)}\n",
    "if \"precipitation\" not in IDX:\n",
    "    raise KeyError(f\"'precipitation' not found in meta['target_cols'] = {target_cols}\")\n",
    "PRECIP_IDX = IDX[\"precipitation\"]\n",
    "n_feat = X0.shape[-1]\n",
    "n_tgt  = Y0.shape[-1]\n",
    "del X0, Y0, T0\n",
    "gc.collect()\n",
    "\n",
    "print(\"[meta] n_feat:\", n_feat, \"n_tgt:\", n_tgt)\n",
    "print(\"[targets]\", target_cols)\n",
    "\n",
    "# ============================================================\n",
    "# 3) SCALER (X only) — computed streaming over train files\n",
    "# ============================================================\n",
    "def compute_x_scaler_stream(files: List[Path], n_feat: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    s1 = np.zeros(n_feat, dtype=np.float64)\n",
    "    s2 = np.zeros(n_feat, dtype=np.float64)\n",
    "    n = 0\n",
    "\n",
    "    for fp in files:\n",
    "        X, _, _, _ = load_npz(fp)\n",
    "        x2 = X.reshape(-1, n_feat).astype(np.float64, copy=False)\n",
    "        s1 += x2.sum(axis=0)\n",
    "        s2 += (x2 * x2).sum(axis=0)\n",
    "        n += x2.shape[0]\n",
    "        del X, x2\n",
    "        gc.collect()\n",
    "\n",
    "    mu = (s1 / max(n,1)).astype(np.float32)\n",
    "    var = (s2 / max(n,1) - (mu.astype(np.float64) ** 2)).astype(np.float64)\n",
    "    var = np.maximum(var, 1e-6)\n",
    "    sd = np.sqrt(var).astype(np.float32)\n",
    "    sd = np.where(sd < 1e-6, 1.0, sd).astype(np.float32)\n",
    "    return mu, sd\n",
    "\n",
    "X_mu, X_sd = compute_x_scaler_stream(TRAIN_FILES, n_feat)\n",
    "np.savez(OUT_DIR / \"x_scaler.npz\",\n",
    "         mu=X_mu, sd=X_sd, input_cols=np.array(input_cols, dtype=object))\n",
    "\n",
    "def scale_x(X: np.ndarray) -> np.ndarray:\n",
    "    return ((X - X_mu[None,None,:]) / X_sd[None,None,:]).astype(np.float32, copy=False)\n",
    "\n",
    "# ============================================================\n",
    "# 4) POS_WEIGHT for event (streaming over train files)\n",
    "# ============================================================\n",
    "def compute_pos_weight_stream(files: List[Path], precip_idx: int, thr_mm: float, cap: float) -> float:\n",
    "    pos = 0.0\n",
    "    total = 0.0\n",
    "    for fp in files:\n",
    "        _, Y, _, _ = load_npz(fp)\n",
    "        y = Y[..., precip_idx]\n",
    "        pos += float((y >= thr_mm).sum())\n",
    "        total += float(y.size)\n",
    "        del Y, y\n",
    "        gc.collect()\n",
    "    neg = total - pos\n",
    "    pw = neg / (pos + 1e-9)\n",
    "    pw = float(min(pw, cap))\n",
    "    return pw\n",
    "\n",
    "POS_WEIGHT = compute_pos_weight_stream(TRAIN_FILES, PRECIP_IDX, cfg.rain_thr_mm, cfg.pos_weight_cap)\n",
    "print(\"[pos_weight]\", POS_WEIGHT)\n",
    "\n",
    "# ============================================================\n",
    "# 5) TORCH + MODEL\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
    "\n",
    "def precip_log_to_mm(pred_log: torch.Tensor, mm_max: float) -> torch.Tensor:\n",
    "    mm = torch.expm1(pred_log).clamp(min=0.0, max=float(mm_max))\n",
    "    return mm\n",
    "\n",
    "class GRUWeather2Step(nn.Module):\n",
    "    def __init__(self, n_feat, n_tgt, horizon, hidden, layers, dropout, precip_idx, amount_space=\"log1p\"):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.n_tgt = n_tgt\n",
    "        self.precip_idx = precip_idx\n",
    "        self.amount_space = amount_space\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=n_feat,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if layers > 1 else 0.0),\n",
    "        )\n",
    "        self.head_main = nn.Linear(hidden, horizon * n_tgt)\n",
    "        self.head_rain = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        h_last = h[-1]\n",
    "        y = self.head_main(h_last).view(-1, self.horizon, self.n_tgt)\n",
    "        rain_logit = self.head_rain(h_last).view(-1, self.horizon)\n",
    "\n",
    "        if self.amount_space == \"mm\":\n",
    "            y[..., self.precip_idx] = F.softplus(y[..., self.precip_idx])\n",
    "        return y, rain_logit\n",
    "\n",
    "def build_optimizer(model: nn.Module):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "def safe_set_weight(w: torch.Tensor, name: str, value: float):\n",
    "    if name in IDX:\n",
    "        w[..., IDX[name]] = float(value)\n",
    "\n",
    "def loss_fn(y_pred, rain_logit, y_true, pos_weight_value: float):\n",
    "    reg = F.smooth_l1_loss(y_pred, y_true, reduction=\"none\")\n",
    "\n",
    "    w = torch.ones((1, 1, y_true.shape[-1]), device=y_true.device, dtype=y_true.dtype)\n",
    "    safe_set_weight(w, \"temperature_2m\", cfg.w_temp)\n",
    "    safe_set_weight(w, \"relative_humidity_2m\", cfg.w_rh)\n",
    "    safe_set_weight(w, \"surface_pressure\", cfg.w_press)\n",
    "    safe_set_weight(w, \"cloud_cover\", cfg.w_cloud)\n",
    "    safe_set_weight(w, \"u10\", cfg.w_wind)\n",
    "    safe_set_weight(w, \"v10\", cfg.w_wind)\n",
    "    w[..., PRECIP_IDX] = 0.0\n",
    "    loss_other = (reg * w).mean()\n",
    "\n",
    "    y_mm = y_true[..., PRECIP_IDX]\n",
    "    rain_label = (y_mm >= cfg.rain_thr_mm).float()\n",
    "    pw = torch.tensor(pos_weight_value, device=y_true.device, dtype=y_true.dtype)\n",
    "    loss_cls = F.binary_cross_entropy_with_logits(rain_logit, rain_label, pos_weight=pw)\n",
    "\n",
    "    pos = rain_label > 0.5\n",
    "    if cfg.amount_space == \"mm\":\n",
    "        pred_mm = y_pred[..., PRECIP_IDX]\n",
    "        if pos.any():\n",
    "            loss_reg = F.smooth_l1_loss(pred_mm[pos], y_mm[pos])\n",
    "        else:\n",
    "            loss_reg = pred_mm.mean() * 0.0\n",
    "    else:\n",
    "        pred_log = y_pred[..., PRECIP_IDX]\n",
    "        true_log = torch.log1p(y_mm.clamp(min=0.0))\n",
    "        if pos.any():\n",
    "            loss_reg = F.smooth_l1_loss(pred_log[pos], true_log[pos])\n",
    "        else:\n",
    "            loss_reg = pred_log.mean() * 0.0\n",
    "\n",
    "    total = loss_other + cfg.loss_cls_w * loss_cls + cfg.loss_reg_w * loss_reg\n",
    "    return total\n",
    "\n",
    "# ============================================================\n",
    "# 6) TRAIN — streaming over files (no concat)\n",
    "# ============================================================\n",
    "def train_gpu_cpu():\n",
    "    device = torch.device(\"cuda\" if (is_gpu() and torch.cuda.is_available()) else \"cpu\")\n",
    "    print(\"[device]\", device)\n",
    "\n",
    "    model = GRUWeather2Step(\n",
    "        n_feat=n_feat, n_tgt=n_tgt, horizon=cfg.horizon,\n",
    "        hidden=cfg.hidden, layers=cfg.layers, dropout=cfg.dropout,\n",
    "        precip_idx=PRECIP_IDX, amount_space=cfg.amount_space\n",
    "    ).to(device)\n",
    "    opt = build_optimizer(model)\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_path = OUT_DIR / \"gru_weather2step_best.pt\"\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        tr_sum = 0.0\n",
    "        tr_n = 0\n",
    "\n",
    "        files = TRAIN_FILES.copy()\n",
    "        np.random.shuffle(files)\n",
    "\n",
    "        for fp in files:\n",
    "            X, Y, _, _ = load_npz(fp)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(\n",
    "                ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                num_workers=(2 if device.type == \"cuda\" else 0),\n",
    "                pin_memory=(device.type == \"cuda\"), drop_last=True\n",
    "            )\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                        yp, rl = model(xb)\n",
    "                        loss = loss_fn(yp, rl, yb, POS_WEIGHT)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                        scaler.unscale_(opt)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    yp, rl = model(xb)\n",
    "                    loss = loss_fn(yp, rl, yb, POS_WEIGHT)\n",
    "                    loss.backward()\n",
    "                    if cfg.grad_clip and cfg.grad_clip > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                    opt.step()\n",
    "\n",
    "                tr_sum += float(loss.item()) * xb.size(0)\n",
    "                tr_n += xb.size(0)\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "        tr_loss = tr_sum / max(tr_n, 1)\n",
    "\n",
    "        model.eval()\n",
    "        va_sum = 0.0\n",
    "        va_n = 0\n",
    "        with torch.no_grad():\n",
    "            for fp in VAL_FILES:\n",
    "                X, Y, _, _ = load_npz(fp)\n",
    "                X = scale_x(X)\n",
    "                ds = ArrayDataset(X, Y)\n",
    "                loader = DataLoader(ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "                for xb, yb in loader:\n",
    "                    xb = xb.to(device, non_blocking=True)\n",
    "                    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=True):\n",
    "                            yp, rl = model(xb)\n",
    "                            loss = loss_fn(yp, rl, yb, POS_WEIGHT)\n",
    "                    else:\n",
    "                        yp, rl = model(xb)\n",
    "                        loss = loss_fn(yp, rl, yb, POS_WEIGHT)\n",
    "\n",
    "                    va_sum += float(loss.item()) * xb.size(0)\n",
    "                    va_n += xb.size(0)\n",
    "\n",
    "                del X, Y, ds, loader\n",
    "                gc.collect()\n",
    "\n",
    "        va_loss = va_sum / max(va_n, 1)\n",
    "\n",
    "        print(f\"epoch {ep:02d} | train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
    "\n",
    "        if va_loss < best_val - 1e-6:\n",
    "            best_val = va_loss\n",
    "            bad = 0\n",
    "            ckpt_meta = dict(meta)\n",
    "            ckpt_meta.update({\n",
    "                \"n_feat\": n_feat,\n",
    "                \"n_tgt\": n_tgt,\n",
    "                \"precip_idx\": PRECIP_IDX,\n",
    "                \"input_cols\": input_cols,\n",
    "                \"target_cols\": target_cols,\n",
    "            })\n",
    "            torch.save(\n",
    "              {\"model\": model.state_dict(), \"cfg\": cfg.__dict__, \"meta\": ckpt_meta, \n",
    "               \"pos_weight\": POS_WEIGHT, \"x_scaler\": {\"mu\": X_mu.tolist(), \"sd\": X_sd.tolist()}},\n",
    "              best_path\n",
    "            )\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= cfg.patience:\n",
    "                print(\"[early stop]\")\n",
    "                break\n",
    "\n",
    "    print(\"[best val]\", best_val, \"->\", best_path)\n",
    "    return best_path.as_posix()\n",
    "\n",
    "# ============================================================\n",
    "# 7) REPORT — streaming (no concat), tune P_THR on VAL, export CSV\n",
    "# ============================================================\n",
    "def event_metrics_counts(y_true01: np.ndarray, y_pred01: np.ndarray):\n",
    "    tp = int(((y_true01 == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true01 == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true01 == 1) & (y_pred01 == 0)).sum())\n",
    "    return tp, fp, fn\n",
    "\n",
    "def tune_pthr_on_val(model, device, amount_space: str) -> Tuple[float, pd.DataFrame]:\n",
    "    cand = np.array(cfg.p_thr_cand, dtype=np.float32)\n",
    "    K = len(cand)\n",
    "    tp = np.zeros(K, dtype=np.int64)\n",
    "    fp = np.zeros(K, dtype=np.int64)\n",
    "    fn = np.zeros(K, dtype=np.int64)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in VAL_FILES:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                yp, logit = model(xb)\n",
    "                p = torch.sigmoid(logit).detach().cpu().numpy().astype(np.float32)\n",
    "                y_mm = yb[..., PRECIP_IDX].detach().cpu().numpy().astype(np.float32)\n",
    "                y_ev = (y_mm >= cfg.rain_thr_mm).reshape(-1)\n",
    "                p1 = p.reshape(-1)\n",
    "\n",
    "                for i, thr in enumerate(cand):\n",
    "                    pred = (p1 >= thr)\n",
    "                    tpi, fpi, fni = event_metrics_counts(y_ev.astype(np.int32), pred.astype(np.int32))\n",
    "                    tp[i] += tpi; fp[i] += fpi; fn[i] += fni\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "    best_i = int(np.argmax(f1))\n",
    "    best_thr = float(cand[best_i])\n",
    "\n",
    "    rank = pd.DataFrame({\n",
    "        \"p_thr\": cand,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn\n",
    "    }).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return best_thr, rank\n",
    "\n",
    "def accumulate_precip_stats(model, device, files: List[Path], p_thr: float, split_name: str, amount_space: str):\n",
    "    H = cfg.horizon\n",
    "    tp = np.zeros(H, dtype=np.int64)\n",
    "    fp = np.zeros(H, dtype=np.int64)\n",
    "    fn = np.zeros(H, dtype=np.int64)\n",
    "    npos = np.zeros(H, dtype=np.int64)\n",
    "\n",
    "    abs_hard = np.zeros(H, dtype=np.float64)\n",
    "    sq_hard  = np.zeros(H, dtype=np.float64)\n",
    "\n",
    "    n_all = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in files:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                yp, logit = model(xb)\n",
    "                p = torch.sigmoid(logit)\n",
    "\n",
    "                y_mm = yb[..., PRECIP_IDX]\n",
    "                y_mm_np = y_mm.detach().cpu().numpy().astype(np.float32)\n",
    "                y_ev_np = (y_mm_np >= cfg.rain_thr_mm)\n",
    "\n",
    "                if amount_space == \"mm\":\n",
    "                    pred_mm = yp[..., PRECIP_IDX].clamp(min=0.0, max=float(cfg.precip_mm_max))\n",
    "                else:\n",
    "                    pred_log = yp[..., PRECIP_IDX]\n",
    "                    pred_mm = precip_log_to_mm(pred_log, cfg.precip_mm_max)\n",
    "\n",
    "                pred_mm_np = pred_mm.detach().cpu().numpy().astype(np.float32)\n",
    "                p_np = p.detach().cpu().numpy().astype(np.float32)\n",
    "                pred_ev_np = (p_np >= p_thr)\n",
    "\n",
    "                tp += (pred_ev_np & y_ev_np).sum(axis=0).astype(np.int64)\n",
    "                fp += (pred_ev_np & (~y_ev_np)).sum(axis=0).astype(np.int64)\n",
    "                fn += ((~pred_ev_np) & y_ev_np).sum(axis=0).astype(np.int64)\n",
    "                npos += y_ev_np.sum(axis=0).astype(np.int64)\n",
    "\n",
    "                hard = np.where(pred_ev_np, pred_mm_np, 0.0).astype(np.float32)\n",
    "                err_h = hard - y_mm_np\n",
    "\n",
    "                abs_hard += np.abs(err_h).sum(axis=0)\n",
    "                sq_hard  += (err_h * err_h).sum(axis=0)\n",
    "\n",
    "                n_all += y_mm_np.shape[0]\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "    denom_all = max(n_all, 1)\n",
    "    mae_hard = abs_hard / denom_all\n",
    "    rmse_hard = np.sqrt(sq_hard / denom_all)\n",
    "\n",
    "    rep = pd.DataFrame({\n",
    "        \"split\": split_name,\n",
    "        \"horizon\": np.arange(1, H+1),\n",
    "        \"rain_thr_mm\": cfg.rain_thr_mm,\n",
    "        \"p_thr\": float(p_thr),\n",
    "        \"event_precision\": prec,\n",
    "        \"event_recall\": rec,\n",
    "        \"event_f1\": f1,\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "        \"npos\": npos,\n",
    "        \"final_mae_all_hard\": mae_hard,\n",
    "        \"final_rmse_all_hard\": rmse_hard,\n",
    "    })\n",
    "    return rep\n",
    "\n",
    "def accumulate_other_targets(model, device, files: List[Path], split_name: str, amount_space: str):\n",
    "    H = cfg.horizon\n",
    "    T = len(target_cols)\n",
    "\n",
    "    targets = [t for t in range(T) if t != PRECIP_IDX]\n",
    "    sum_abs = np.zeros((len(targets), H), dtype=np.float64)\n",
    "    sum_sq  = np.zeros((len(targets), H), dtype=np.float64)\n",
    "    n_all = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for fp_npz in files:\n",
    "            X, Y, _, _ = load_npz(fp_npz)\n",
    "            X = scale_x(X)\n",
    "            ds = ArrayDataset(X, Y)\n",
    "            loader = DataLoader(ds, batch_size=512, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "            for xb, yb in loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                yp, _ = model(xb)\n",
    "\n",
    "                y_true = yb.detach().cpu().numpy().astype(np.float32)\n",
    "                y_pred = yp.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "                for i, ti in enumerate(targets):\n",
    "                    de = (y_pred[:, :, ti] - y_true[:, :, ti]).astype(np.float64)\n",
    "                    sum_abs[i] += np.abs(de).sum(axis=0)\n",
    "                    sum_sq[i]  += (de * de).sum(axis=0)\n",
    "\n",
    "                n_all += y_true.shape[0]\n",
    "\n",
    "            del X, Y, ds, loader\n",
    "            gc.collect()\n",
    "\n",
    "    denom = max(n_all, 1)\n",
    "    rows = []\n",
    "    for i, ti in enumerate(targets):\n",
    "        name = target_cols[ti]\n",
    "        mae = (sum_abs[i] / denom).astype(np.float64)\n",
    "        rmse = np.sqrt(sum_sq[i] / denom).astype(np.float64)\n",
    "        for h in range(H):\n",
    "            rows.append({\n",
    "                \"split\": split_name,\n",
    "                \"target\": name,\n",
    "                \"horizon\": h+1,\n",
    "                \"mae\": float(mae[h]),\n",
    "                \"rmse\": float(rmse[h]),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_bins_precip(rep_split: pd.DataFrame):\n",
    "    rows = []\n",
    "    for a,b in cfg.bins:\n",
    "        r = rep_split[(rep_split[\"horizon\"]>=a) & (rep_split[\"horizon\"]<=b)]\n",
    "        if len(r) == 0:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"split\": r[\"split\"].iloc[0],\n",
    "            \"horizon_bin\": f\"{a}-{b}\",\n",
    "            \"event_f1_mean\": float(r[\"event_f1\"].mean()),\n",
    "            \"event_recall_mean\": float(r[\"event_recall\"].mean()),\n",
    "            \"event_precision_mean\": float(r[\"event_precision\"].mean()),\n",
    "            \"mae_all_hard_mean\": float(r[\"final_mae_all_hard\"].mean()),\n",
    "            \"rmse_all_hard_mean\": float(r[\"final_rmse_all_hard\"].mean()),\n",
    "            \"npos_sum\": int(r[\"npos\"].sum()),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_bins_other(rep_long: pd.DataFrame):\n",
    "    rows = []\n",
    "    for (split, target), g in rep_long.groupby([\"split\", \"target\"]):\n",
    "        for a,b in cfg.bins:\n",
    "            r = g[(g[\"horizon\"]>=a) & (g[\"horizon\"]<=b)]\n",
    "            if len(r) == 0:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"split\": split,\n",
    "                \"target\": target,\n",
    "                \"horizon_bin\": f\"{a}-{b}\",\n",
    "                \"mae_mean\": float(r[\"mae\"].mean()),\n",
    "                \"rmse_mean\": float(r[\"rmse\"].mean()),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def run_report(best_ckpt_path: Path):\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    cfg_ck = ckpt.get(\"cfg\", {})\n",
    "    amount_space = str(cfg_ck.get(\"amount_space\", cfg.amount_space))\n",
    "\n",
    "    model = GRUWeather2Step(\n",
    "        n_feat=n_feat, n_tgt=n_tgt, horizon=cfg.horizon,\n",
    "        hidden=int(cfg_ck.get(\"hidden\", cfg.hidden)),\n",
    "        layers=int(cfg_ck.get(\"layers\", cfg.layers)),\n",
    "        dropout=float(cfg_ck.get(\"dropout\", cfg.dropout)),\n",
    "        precip_idx=PRECIP_IDX,\n",
    "        amount_space=amount_space\n",
    "    )\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    best_p_thr, thr_rank = tune_pthr_on_val(model, device, amount_space)\n",
    "    thr_rank.to_csv(REPORT_DIR / \"precip_thr_tuning_val.csv\", index=False)\n",
    "    print(\"[REPORT] best_p_thr (VAL F1) =\", best_p_thr)\n",
    "\n",
    "    rep_p_val = accumulate_precip_stats(model, device, VAL_FILES, best_p_thr, \"val\", amount_space)\n",
    "    rep_p_te  = accumulate_precip_stats(model, device, TEST_FILES, best_p_thr, \"test\", amount_space)\n",
    "    rep_p = pd.concat([rep_p_val, rep_p_te], ignore_index=True)\n",
    "    rep_p.to_csv(REPORT_DIR / f\"gru_precip_report_val_test_h1-{cfg.horizon}.csv\", index=False)\n",
    "\n",
    "    sum_p = pd.concat([summarize_bins_precip(rep_p_val), summarize_bins_precip(rep_p_te)], ignore_index=True)\n",
    "    sum_p[\"p_thr\"] = best_p_thr\n",
    "    sum_p[\"rain_thr_mm\"] = cfg.rain_thr_mm\n",
    "    sum_p.to_csv(REPORT_DIR / \"gru_precip_summary_bins_val_test.csv\", index=False)\n",
    "\n",
    "    rep_o_val = accumulate_other_targets(model, device, VAL_FILES, \"val\", amount_space)\n",
    "    rep_o_te  = accumulate_other_targets(model, device, TEST_FILES, \"test\", amount_space)\n",
    "    rep_o = pd.concat([rep_o_val, rep_o_te], ignore_index=True)\n",
    "    rep_o.to_csv(REPORT_DIR / \"gru_other_targets_report_val_test_long.csv\", index=False)\n",
    "\n",
    "    sum_o = summarize_bins_other(rep_o)\n",
    "    sum_o.to_csv(REPORT_DIR / \"gru_other_targets_summary_bins_val_test.csv\", index=False)\n",
    "\n",
    "    print(\"[REPORT] saved in:\", REPORT_DIR)\n",
    "\n",
    "# ============================================================\n",
    "# 8) MAIN\n",
    "# ============================================================\n",
    "def export_traced_gru(best_ckpt_path: Path, out_dir: Path):\n",
    "    best_ckpt_path = Path(best_ckpt_path)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    obj = torch.load(str(best_ckpt_path), map_location=\"cpu\", weights_only=False)\n",
    "    sd = obj[\"model\"]\n",
    "    cfg_d = obj.get(\"cfg\", {})\n",
    "    meta_d = obj.get(\"meta\", {})\n",
    "\n",
    "    lag = int(cfg.lag)\n",
    "    horizon = int(cfg.horizon)\n",
    "\n",
    "    if \"gru.weight_ih_l0\" in sd:\n",
    "        n_feat_from_sd = sd[\"gru.weight_ih_l0\"].shape[1]\n",
    "    else:\n",
    "        n_feat_from_sd = None\n",
    "    n_feat_e = int(n_feat_from_sd or meta_d.get(\"n_feat\", 12))\n",
    "\n",
    "    if \"head_main.weight\" in sd:\n",
    "        n_tgt_from_sd = sd[\"head_main.weight\"].shape[0] // horizon\n",
    "    else:\n",
    "        n_tgt_from_sd = None\n",
    "    n_tgt_e = int(n_tgt_from_sd or meta_d.get(\"n_tgt\", 7))\n",
    "\n",
    "    precip_idx_e = int(meta_d.get(\"precip_idx\", 1))\n",
    "\n",
    "    hidden = int(cfg_d.get(\"hidden\", 192))\n",
    "    layers = int(cfg_d.get(\"layers\", 2))\n",
    "    dropout = float(cfg_d.get(\"dropout\", 0.0))\n",
    "    amount_space = str(cfg_d.get(\"amount_space\", \"log1p\"))\n",
    "\n",
    "    m = GRUWeather2Step(\n",
    "        n_feat=n_feat_e, n_tgt=n_tgt_e, horizon=horizon,\n",
    "        hidden=hidden, layers=layers, dropout=dropout,\n",
    "        precip_idx=precip_idx_e, amount_space=amount_space\n",
    "    )\n",
    "    m.load_state_dict(sd, strict=True)\n",
    "    m.eval()\n",
    "\n",
    "    x = torch.zeros(1, lag, n_feat_e, dtype=torch.float32)\n",
    "    _ = m(x)\n",
    "\n",
    "    traced = torch.jit.trace(m, x, strict=False)\n",
    "    ts_path = out_dir / \"gru_weather2step_best_traced.pt\"\n",
    "    traced.save(str(ts_path))\n",
    "    print(\"[EXPORT] GRU traced saved:\", ts_path)\n",
    "    return ts_path\n",
    "\n",
    "def main():\n",
    "    print(\"ACCELERATOR:\", ACCELERATOR)\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(f\"CONFIG: LAG={cfg.lag}, HORIZON={cfg.horizon}, HIDDEN={cfg.hidden}\")\n",
    "\n",
    "    best_ckpt = Path(train_gpu_cpu())\n",
    "\n",
    "    if cfg.run_report:\n",
    "        run_report(best_ckpt)\n",
    "\n",
    "    try:\n",
    "        ts_path = export_traced_gru(best_ckpt, OUT_DIR)\n",
    "        _m = torch.jit.load(str(ts_path), map_location=\"cpu\")\n",
    "        _m.eval()\n",
    "        print(\"[EXPORT][VERIFY] torch.jit.load OK:\", ts_path)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] TorchScript export failed:\", e)\n",
    "    \n",
    "    print(\"DONE. out_dir =\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
