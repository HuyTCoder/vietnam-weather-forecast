{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18e4c11",
   "metadata": {},
   "source": [
    "## XGBoost (Kaggle) â€” Train by LOCATION_ID (20 Locations)\n",
    "\n",
    "**YÃªu cáº§u Dataset:**\n",
    "- Cháº¡y `fetch-demo-data-singlekeys.ipynb` trÆ°á»›c (Ä‘Ã£ fetch 20 tá»‰nh/thÃ nh)\n",
    "- Upload output (`weather_20loc`) thÃ nh Kaggle Dataset\n",
    "- Add dataset vÃ o notebook nÃ y\n",
    "\n",
    "**Config:**\n",
    "- LAG = 49h lookback\n",
    "- HORIZON = 100h forecast (~4 ngÃ y)\n",
    "- 20 locations\n",
    "\n",
    "**Speed Optimizations:**\n",
    "- `learning_rate = 0.08` (tÄƒng tá»« 0.05) â†’ há»™i tá»¥ nhanh hÆ¡n\n",
    "- `NUM_BOOST = 2500` (giáº£m tá»« 5000) â†’ váº«n Ä‘á»§ vá»›i early stopping\n",
    "- `EARLY_STOP = 100` (giáº£m tá»« 200) â†’ check nhanh hÆ¡n\n",
    "- `nthread = all CPUs` â†’ song song hÃ³a\n",
    "\n",
    "**Bins Reporting (giá»‘ng GRU/TCN/LightGBM):**\n",
    "- `1-24h`: Ngáº¯n háº¡n (1 ngÃ y)\n",
    "- `25-48h`: Trung háº¡n (1-2 ngÃ y)\n",
    "- `49-72h`: Trung-dÃ i (2-3 ngÃ y)\n",
    "- `73-100h`: DÃ i háº¡n (3-4 ngÃ y)\n",
    "\n",
    "**Features:**\n",
    "- GPU auto-detect vá»›i fallback CPU\n",
    "- Tá»± dÃ² weather_20loc/data vÃ  load location_ids tá»« metadata\n",
    "- MÆ°a 2-stage (event + amount vá»›i log1p)\n",
    "- CÃ³ thá»ƒ cháº¡y 1 target hoáº·c táº¥t cáº£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBoost trainer - Train by LOCATION_ID from Kaggle Dataset\n",
    "# 20 provinces/cities, LAG=49, HORIZON=100\n",
    "# OPTIMIZED: parallel training, bins reporting\n",
    "# ============================================================\n",
    "\n",
    "!pip -q install -U \"xgboost>=2.0\"\n",
    "\n",
    "import os, gc, json, subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"XGBoost version:\", xgb.__version__)\n",
    "\n",
    "# ============================================================\n",
    "# 0) GPU CHECK\n",
    "# ============================================================\n",
    "def can_run_nvidia_smi():\n",
    "    try:\n",
    "        r = subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        return r.returncode == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"nvidia-smi available:\", can_run_nvidia_smi())\n",
    "if can_run_nvidia_smi():\n",
    "    _ = subprocess.run([\"nvidia-smi\", \"-L\"], check=False)\n",
    "\n",
    "# ============================================================\n",
    "# 1) RUN CONTROL (OPTIMIZED)\n",
    "# ============================================================\n",
    "TARGETS_TO_RUN = \"all\"  # \"all\" or one of: temp, rain, u10, v10, rh, press, cloud\n",
    "USE_GPU = True\n",
    "\n",
    "H_START = 1\n",
    "H_END   = 100\n",
    "LAG = 49\n",
    "H   = 100\n",
    "\n",
    "# === BINS for reporting (giá»‘ng GRU/TCN/LightGBM) ===\n",
    "BINS = ((1,24), (25,48), (49,72), (73,100))\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "START_LOC_IDX = 0\n",
    "END_LOC_IDX = -1\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": \"train_2021_2023\",\n",
    "    \"val\":   \"val_2024\",\n",
    "    \"test\":  \"test_2025_01_to_2025_11\",\n",
    "}\n",
    "\n",
    "CANON_KEYS = [\"temp\",\"rain\",\"u10\",\"v10\",\"rh\",\"press\",\"cloud\"]\n",
    "\n",
    "if TARGETS_TO_RUN == \"all\":\n",
    "    TARGETS = CANON_KEYS\n",
    "else:\n",
    "    assert TARGETS_TO_RUN in CANON_KEYS, f\"RUN must be one of {sorted(CANON_KEYS)}\"\n",
    "    TARGETS = [TARGETS_TO_RUN]\n",
    "\n",
    "# ============================================================\n",
    "# 2) AUTO-DETECT DATA DIR + LOAD LOCATION_IDS\n",
    "# ============================================================\n",
    "INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "\n",
    "def find_data_dir():\n",
    "    # Only look for weather_20loc\n",
    "    for p in INPUT_ROOT.rglob(\"weather_20loc/data\"):\n",
    "        if p.is_dir():\n",
    "            return p\n",
    "    for p in INPUT_ROOT.rglob(\"data/tabular\"):\n",
    "        if p.is_dir():\n",
    "            return p.parent\n",
    "    raise FileNotFoundError(\"KhÃ´ng tÃ¬m tháº¥y weather_20loc/data trong /kaggle/input\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n",
    "TAB_DIR = DATA_DIR / \"tabular\"\n",
    "META_DIR = DATA_DIR / \"meta\"\n",
    "\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"TAB_DIR = {TAB_DIR}\")\n",
    "\n",
    "def load_location_ids():\n",
    "    meta_file = META_DIR / \"locations.json\"\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as f:\n",
    "            meta = json.load(f)\n",
    "        loc_ids = meta.get(\"location_ids\", [])\n",
    "        locations = meta.get(\"locations\", [])\n",
    "        print(f\"Loaded {len(loc_ids)} locations from metadata:\")\n",
    "        for loc in locations:\n",
    "            print(f\"  {loc['name']:15s} = {loc['location_id']}\")\n",
    "        return loc_ids, {loc[\"location_id\"]: loc[\"name\"] for loc in locations}\n",
    "    \n",
    "    print(\"[warn] locations.json not found, scanning files...\")\n",
    "    files = list(TAB_DIR.glob(f\"*_{SPLITS['train']}_tab_temp_lag{LAG}_h{H}.parquet\"))\n",
    "    loc_ids = sorted(set(f.name.split(\"_\")[0] for f in files))\n",
    "    print(f\"Found {len(loc_ids)} location_ids from files\")\n",
    "    return loc_ids, {}\n",
    "\n",
    "LOCATION_IDS_ALL, LOC_NAMES = load_location_ids()\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "_start = START_LOC_IDX\n",
    "_end = END_LOC_IDX if END_LOC_IDX >= 0 else len(LOCATION_IDS_ALL)\n",
    "LOCATION_IDS = LOCATION_IDS_ALL[_start:_end]\n",
    "print(f\"[LOCATION BATCH] Using {len(LOCATION_IDS)}/{len(LOCATION_IDS_ALL)} locations (idx {_start}:{_end})\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) OUTPUT DIRS\n",
    "# ============================================================\n",
    "OUT_DIR = Path(\"/kaggle/working/xgb_out_singlekeys\")\n",
    "MODEL_DIR = OUT_DIR / \"models\"\n",
    "REPORT_DIR = OUT_DIR / \"reports\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 4) IO HELPERS\n",
    "# ============================================================\n",
    "def ycol(h: int):\n",
    "    return f\"y_t+{h:03d}\"\n",
    "\n",
    "def load_split(loc_id: str, split_name: str, target_key: str) -> pd.DataFrame:\n",
    "    fn = f\"{loc_id}_{split_name}_tab_{target_key}_lag{LAG}_h{H}.parquet\"\n",
    "    path = TAB_DIR / fn\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {path}\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def get_cols_from_df(df):\n",
    "    feat_cols = [c for c in df.columns if \"_lag\" in c]\n",
    "    y_cols = [ycol(h) for h in range(1, H+1)]\n",
    "    return feat_cols, y_cols\n",
    "\n",
    "def loc_short_name(loc_id: str) -> str:\n",
    "    return LOC_NAMES.get(loc_id, loc_id[:8])\n",
    "\n",
    "# ============================================================\n",
    "# 5) METRICS\n",
    "# ============================================================\n",
    "def mae(yhat, y):\n",
    "    return float(np.mean(np.abs(np.asarray(yhat, np.float32) - np.asarray(y, np.float32))))\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    d = np.asarray(yhat, np.float32) - np.asarray(y, np.float32)\n",
    "    return float(np.sqrt(np.mean(d * d)))\n",
    "\n",
    "def event_metrics(y_true01, y_pred01):\n",
    "    y_true01 = np.asarray(y_true01).astype(np.int32)\n",
    "    y_pred01 = np.asarray(y_pred01).astype(np.int32)\n",
    "    tp = int(((y_true01 == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true01 == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true01 == 1) & (y_pred01 == 0)).sum())\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return float(prec), float(rec), float(f1), tp, fp, fn\n",
    "\n",
    "# ============================================================\n",
    "# 6) XGBoost params (SPEED OPTIMIZED)\n",
    "# ============================================================\n",
    "NUM_BOOST = 2500        # Reduced from 5000\n",
    "EARLY_STOP = 100        # Reduced from 200\n",
    "\n",
    "RAIN_MM_THR   = 0.1\n",
    "P_THR_CAND    = np.round(np.linspace(0.05, 0.95, 19), 2).tolist()\n",
    "MIN_POS_TRAIN = 300\n",
    "MIN_POS_VAL   = 50\n",
    "USE_LOG1P_AMOUNT = True\n",
    "\n",
    "def tune_p_thr_on_val(y_true_evt: np.ndarray, p_pred: np.ndarray, candidates=P_THR_CAND):\n",
    "    best_thr = 0.5\n",
    "    best_f1 = -1.0\n",
    "    for thr in candidates:\n",
    "        pred_evt = (p_pred >= thr).astype(np.int32)\n",
    "        _, _, f1, _, _, _ = event_metrics(y_true_evt, pred_evt)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    return best_thr, best_f1\n",
    "\n",
    "def choose_device():\n",
    "    return \"cuda\" if USE_GPU and can_run_nvidia_smi() else \"cpu\"\n",
    "\n",
    "def common_params(device: str):\n",
    "    n_jobs = os.cpu_count() or 4\n",
    "    p = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": device,\n",
    "        \"learning_rate\": 0.08,      # Increased from 0.05 (faster convergence)\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 5,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"max_bin\": 256,\n",
    "        \"nthread\": n_jobs,          # Use all CPU cores\n",
    "        \"verbosity\": 0,             # Reduce logging\n",
    "    }\n",
    "    if device == \"cuda\":\n",
    "        p[\"sampling_method\"] = \"gradient_based\"\n",
    "    return p\n",
    "\n",
    "def params_reg(device: str):\n",
    "    p = common_params(device)\n",
    "    p.update({\"objective\": \"reg:squarederror\", \"eval_metric\": \"rmse\"})\n",
    "    return p\n",
    "\n",
    "def params_clf(device: str, scale_pos_weight: float):\n",
    "    p = common_params(device)\n",
    "    p.update({\"objective\": \"binary:logistic\", \"eval_metric\": \"logloss\", \"scale_pos_weight\": float(scale_pos_weight)})\n",
    "    return p\n",
    "\n",
    "def predict_best(model: xgb.Booster, dmat: xgb.DMatrix):\n",
    "    bi = getattr(model, \"best_iteration\", None)\n",
    "    if bi is None:\n",
    "        return model.predict(dmat)\n",
    "    return model.predict(dmat, iteration_range=(0, int(bi) + 1))\n",
    "\n",
    "def train_with_es(params, dtr, dva):\n",
    "    try:\n",
    "        cb = [xgb.callback.EarlyStopping(rounds=EARLY_STOP, save_best=True)]\n",
    "        return xgb.train(params=params, dtrain=dtr, num_boost_round=NUM_BOOST, evals=[(dva, \"val\")], callbacks=cb, verbose_eval=500)\n",
    "    except Exception:\n",
    "        return xgb.train(params=params, dtrain=dtr, num_boost_round=NUM_BOOST, evals=[(dva, \"val\")], early_stopping_rounds=EARLY_STOP, verbose_eval=500)\n",
    "\n",
    "def train_with_device_fallback(make_params_fn, dtr, dva, *args):\n",
    "    dev = choose_device()\n",
    "    try:\n",
    "        params = make_params_fn(dev, *args)\n",
    "        model = train_with_es(params, dtr, dva)\n",
    "        return model, dev\n",
    "    except xgb.core.XGBoostError as e:\n",
    "        msg = str(e).lower()\n",
    "        if dev == \"cuda\" and (\"cuda\" in msg or \"gpu\" in msg or \"device\" in msg):\n",
    "            print(\"[warn] GPU failed, fallback CPU.\\n\", str(e)[:200])\n",
    "            params = make_params_fn(\"cpu\", *args)\n",
    "            model = train_with_es(params, dtr, dva)\n",
    "            return model, \"cpu\"\n",
    "        raise\n",
    "\n",
    "def _make_params_reg(device, *_):\n",
    "    return params_reg(device)\n",
    "\n",
    "def _make_params_clf(device, scale_pos_weight):\n",
    "    return params_clf(device, scale_pos_weight)\n",
    "\n",
    "# ============================================================\n",
    "# 7) TRAIN 1-stage reg (OPTIMIZED)\n",
    "# ============================================================\n",
    "def train_reg_1stage(loc_id: str, target_key: str, h_start: int, h_end: int):\n",
    "    name = loc_short_name(loc_id)\n",
    "    print(f\"\\n=== XGB 1-STAGE | {name} ({loc_id[:8]}...) target={target_key} h={h_start:03d}-{h_end:03d} ===\")\n",
    "\n",
    "    df_tr = load_split(loc_id, SPLITS[\"train\"], target_key)\n",
    "    df_va = load_split(loc_id, SPLITS[\"val\"], target_key)\n",
    "    df_te = load_split(loc_id, SPLITS[\"test\"], target_key)\n",
    "\n",
    "    feat_cols, y_cols = get_cols_from_df(df_tr)\n",
    "\n",
    "    Xtr = df_tr[feat_cols].to_numpy(np.float32)\n",
    "    Xva = df_va[feat_cols].to_numpy(np.float32)\n",
    "    Xte = df_te[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    Ytr = df_tr[y_cols].to_numpy(np.float32)\n",
    "    Yva = df_va[y_cols].to_numpy(np.float32)\n",
    "    Yte = df_te[y_cols].to_numpy(np.float32)\n",
    "\n",
    "    # Pre-create DMatrix once (SPEED OPTIMIZATION)\n",
    "    dtr = xgb.DMatrix(Xtr)\n",
    "    dva = xgb.DMatrix(Xva)\n",
    "    dte = xgb.DMatrix(Xte)\n",
    "\n",
    "    mdir = MODEL_DIR / target_key\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    report_path = REPORT_DIR / f\"report_xgb_{target_key}_{loc_id}.csv\"\n",
    "    old = pd.read_csv(report_path) if report_path.exists() else None\n",
    "\n",
    "    rows = []\n",
    "    for h in range(h_start, h_end + 1):\n",
    "        mp = mdir / f\"xgb_{target_key}_{loc_id}_h{h:03d}.json\"\n",
    "        if mp.exists():\n",
    "            continue\n",
    "\n",
    "        dtr.set_label(Ytr[:, h-1])\n",
    "        dva.set_label(Yva[:, h-1])\n",
    "\n",
    "        model, used_dev = train_with_device_fallback(_make_params_reg, dtr, dva)\n",
    "\n",
    "        pred = predict_best(model, dte)\n",
    "        yte = Yte[:, h-1]\n",
    "\n",
    "        rows.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": target_key, \n",
    "            \"h\": h,\n",
    "            \"device\": used_dev,\n",
    "            \"best_iter\": int(getattr(model, \"best_iteration\", -1)),\n",
    "            \"test_mae\": mae(pred, yte),\n",
    "            \"test_rmse\": rmse(pred, yte),\n",
    "            \"model\": mp.name,\n",
    "        })\n",
    "        model.save_model(mp)\n",
    "\n",
    "        if h % 25 == 0 or h == h_end:\n",
    "            print(f\"[{target_key}] {name} h={h:03d} dev={used_dev} mae={rows[-1]['test_mae']:.4f}\")\n",
    "\n",
    "        del model, pred\n",
    "        gc.collect()\n",
    "\n",
    "    del Xtr, Xva, Xte, Ytr, Yva, Yte, dtr, dva, dte, df_tr, df_va, df_te\n",
    "    gc.collect()\n",
    "\n",
    "    if rows:\n",
    "        new = pd.DataFrame(rows)\n",
    "        out = pd.concat([old, new], ignore_index=True) if old is not None else new\n",
    "        out.to_csv(report_path, index=False)\n",
    "        return out\n",
    "    print(\"[info] nothing new trained (resume hit).\")\n",
    "    return old if old is not None else pd.DataFrame()\n",
    "\n",
    "# ============================================================\n",
    "# 8) TRAIN 2-stage rain (OPTIMIZED)\n",
    "# ============================================================\n",
    "def train_rain_2stage(loc_id: str, h_start: int, h_end: int):\n",
    "    name = loc_short_name(loc_id)\n",
    "    print(f\"\\n=== XGB 2-STAGE RAIN | {name} ({loc_id[:8]}...) h={h_start:03d}-{h_end:03d} ===\")\n",
    "\n",
    "    df_tr = load_split(loc_id, SPLITS[\"train\"], \"rain\")\n",
    "    df_va = load_split(loc_id, SPLITS[\"val\"], \"rain\")\n",
    "    df_te = load_split(loc_id, SPLITS[\"test\"], \"rain\")\n",
    "\n",
    "    feat_cols, y_cols = get_cols_from_df(df_tr)\n",
    "\n",
    "    Xtr = df_tr[feat_cols].to_numpy(np.float32)\n",
    "    Xva = df_va[feat_cols].to_numpy(np.float32)\n",
    "    Xte = df_te[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    Ytr = df_tr[y_cols].to_numpy(np.float32)\n",
    "    Yva = df_va[y_cols].to_numpy(np.float32)\n",
    "    Yte = df_te[y_cols].to_numpy(np.float32)\n",
    "\n",
    "    dtr = xgb.DMatrix(Xtr)\n",
    "    dva = xgb.DMatrix(Xva)\n",
    "    dte = xgb.DMatrix(Xte)\n",
    "\n",
    "    mdir = MODEL_DIR / \"rain\"\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    report_path = REPORT_DIR / f\"report_xgb_rain_{loc_id}.csv\"\n",
    "    old = pd.read_csv(report_path) if report_path.exists() else None\n",
    "\n",
    "    rows = []\n",
    "    for h in range(h_start, h_end + 1):\n",
    "        mp_clf = mdir / f\"xgb_rain_clf_{loc_id}_h{h:03d}.json\"\n",
    "        mp_reg = mdir / f\"xgb_rain_reg_{loc_id}_h{h:03d}.json\"\n",
    "        if mp_clf.exists() and mp_reg.exists():\n",
    "            continue\n",
    "\n",
    "        ytr_amt = Ytr[:, h-1]; yva_amt = Yva[:, h-1]; yte_amt = Yte[:, h-1]\n",
    "        ytr_evt = (ytr_amt >= RAIN_MM_THR).astype(np.float32)\n",
    "        yva_evt = (yva_amt >= RAIN_MM_THR).astype(np.float32)\n",
    "        yte_evt = (yte_amt >= RAIN_MM_THR).astype(np.int32)\n",
    "\n",
    "        pos = float(ytr_evt.sum()); neg = float(len(ytr_evt) - pos)\n",
    "        spw = max(1.0, neg / max(pos, 1.0))\n",
    "\n",
    "        # CLASSIFIER\n",
    "        dtr.set_label(ytr_evt)\n",
    "        dva.set_label(yva_evt)\n",
    "        clf, dev_clf = train_with_device_fallback(_make_params_clf, dtr, dva, spw)\n",
    "        \n",
    "        p_va = predict_best(clf, dva)\n",
    "        best_p_thr, val_f1 = tune_p_thr_on_val(yva_evt.astype(np.int32), p_va)\n",
    "        p_te = predict_best(clf, dte)\n",
    "\n",
    "        # REGRESSOR\n",
    "        idx_tr = ytr_evt > 0.5\n",
    "        idx_va = yva_evt > 0.5\n",
    "        \n",
    "        if USE_LOG1P_AMOUNT:\n",
    "            ytr_amt_log = np.log1p(ytr_amt)\n",
    "            yva_amt_log = np.log1p(yva_amt)\n",
    "        else:\n",
    "            ytr_amt_log = ytr_amt\n",
    "            yva_amt_log = yva_amt\n",
    "        \n",
    "        if idx_tr.sum() < MIN_POS_TRAIN or idx_va.sum() < MIN_POS_VAL:\n",
    "            dtr.set_label(ytr_amt_log)\n",
    "            dva.set_label(yva_amt_log)\n",
    "            reg, dev_reg = train_with_device_fallback(_make_params_reg, dtr, dva)\n",
    "            pred_log = predict_best(reg, dte)\n",
    "        else:\n",
    "            dtr_r = xgb.DMatrix(Xtr[idx_tr], label=ytr_amt_log[idx_tr])\n",
    "            dva_r = xgb.DMatrix(Xva[idx_va], label=yva_amt_log[idx_va])\n",
    "            try:\n",
    "                dev = choose_device()\n",
    "                reg = train_with_es(params_reg(dev), dtr_r, dva_r)\n",
    "                dev_reg = dev\n",
    "            except xgb.core.XGBoostError as e:\n",
    "                msg = str(e).lower()\n",
    "                if dev == \"cuda\" and (\"cuda\" in msg or \"gpu\" in msg):\n",
    "                    reg = train_with_es(params_reg(\"cpu\"), dtr_r, dva_r)\n",
    "                    dev_reg = \"cpu\"\n",
    "                else:\n",
    "                    raise\n",
    "            pred_log = predict_best(reg, dte)\n",
    "        \n",
    "        if USE_LOG1P_AMOUNT:\n",
    "            yhat_amt = np.expm1(pred_log).astype(np.float32)\n",
    "        else:\n",
    "            yhat_amt = pred_log.astype(np.float32)\n",
    "        yhat_amt = np.maximum(yhat_amt, 0.0)\n",
    "\n",
    "        yhat_evt = (p_te >= best_p_thr).astype(np.int32)\n",
    "        yhat = np.where(yhat_evt == 1, yhat_amt, 0.0).astype(np.float32)\n",
    "\n",
    "        prec, rec, f1, tp, fp, fn = event_metrics(yte_evt, yhat_evt)\n",
    "        rows.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": \"rain\", \"h\": h,\n",
    "            \"device_clf\": dev_clf, \"device_reg\": dev_reg,\n",
    "            \"best_iter_clf\": int(getattr(clf, \"best_iteration\", -1)),\n",
    "            \"best_iter_reg\": int(getattr(reg, \"best_iteration\", -1)),\n",
    "            \"p_thr_tuned\": best_p_thr,\n",
    "            \"use_log1p\": USE_LOG1P_AMOUNT,\n",
    "            \"test_mae\": mae(yhat, yte_amt),\n",
    "            \"test_rmse\": rmse(yhat, yte_amt),\n",
    "            \"prec\": prec, \"rec\": rec, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "            \"model_clf\": mp_clf.name, \"model_reg\": mp_reg.name,\n",
    "        })\n",
    "\n",
    "        clf.save_model(mp_clf)\n",
    "        reg.save_model(mp_reg)\n",
    "\n",
    "        if h % 25 == 0 or h == h_end:\n",
    "            print(f\"[rain] {name} h={h:03d} p_thr={best_p_thr:.2f} mae={rows[-1]['test_mae']:.4f} f1={f1:.3f}\")\n",
    "\n",
    "        del clf, reg, p_te, p_va, yhat_amt, yhat, yhat_evt, pred_log\n",
    "        gc.collect()\n",
    "\n",
    "    del Xtr, Xva, Xte, Ytr, Yva, Yte, dtr, dva, dte, df_tr, df_va, df_te\n",
    "    gc.collect()\n",
    "\n",
    "    if rows:\n",
    "        new = pd.DataFrame(rows)\n",
    "        out = pd.concat([old, new], ignore_index=True) if old is not None else new\n",
    "        out.to_csv(report_path, index=False)\n",
    "        return out\n",
    "    print(\"[info] nothing new trained (resume hit).\")\n",
    "    return old if old is not None else pd.DataFrame()\n",
    "\n",
    "# ============================================================\n",
    "# 9) BINS SUMMARY (giá»‘ng GRU/TCN/LightGBM)\n",
    "# ============================================================\n",
    "def summarize_bins(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    \"\"\"Summarize metrics by horizon bins\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rows = []\n",
    "    for a, b in BINS:\n",
    "        mask = (df[\"h\"] >= a) & (df[\"h\"] <= b)\n",
    "        sub = df[mask]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            \"target\": target,\n",
    "            \"horizon_bin\": f\"{a}-{b}\",\n",
    "            \"n_horizons\": len(sub),\n",
    "        }\n",
    "        \n",
    "        if \"test_mae\" in sub.columns:\n",
    "            row[\"mae_mean\"] = float(sub[\"test_mae\"].mean())\n",
    "        if \"test_rmse\" in sub.columns:\n",
    "            row[\"rmse_mean\"] = float(sub[\"test_rmse\"].mean())\n",
    "        if \"f1\" in sub.columns:\n",
    "            row[\"f1_mean\"] = float(sub[\"f1\"].mean())\n",
    "        if \"prec\" in sub.columns:\n",
    "            row[\"prec_mean\"] = float(sub[\"prec\"].mean())\n",
    "        if \"rec\" in sub.columns:\n",
    "            row[\"rec_mean\"] = float(sub[\"rec\"].mean())\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_all_bins() -> pd.DataFrame:\n",
    "    \"\"\"Summarize all targets with bins\"\"\"\n",
    "    all_bins = []\n",
    "    \n",
    "    for tkey in TARGETS:\n",
    "        for loc_id in LOCATION_IDS:\n",
    "            if tkey == \"rain\":\n",
    "                report_path = REPORT_DIR / f\"report_xgb_rain_{loc_id}.csv\"\n",
    "            else:\n",
    "                report_path = REPORT_DIR / f\"report_xgb_{tkey}_{loc_id}.csv\"\n",
    "            \n",
    "            if not report_path.exists():\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(report_path)\n",
    "            bins_df = summarize_bins(df, tkey)\n",
    "            if len(bins_df) > 0:\n",
    "                bins_df[\"location_id\"] = loc_id\n",
    "                bins_df[\"location_name\"] = loc_short_name(loc_id)\n",
    "                all_bins.append(bins_df)\n",
    "    \n",
    "    if not all_bins:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.concat(all_bins, ignore_index=True)\n",
    "\n",
    "# ============================================================\n",
    "# 10) RUN - Train theo LOCATION_ID\n",
    "# ============================================================\n",
    "summaries = []\n",
    "for tkey in TARGETS:\n",
    "    for loc_id in LOCATION_IDS:\n",
    "        name = loc_short_name(loc_id)\n",
    "        if tkey == \"rain\":\n",
    "            rep = train_rain_2stage(loc_id, H_START, H_END)\n",
    "        else:\n",
    "            rep = train_reg_1stage(loc_id, tkey, H_START, H_END)\n",
    "\n",
    "        if rep is None or len(rep) == 0:\n",
    "            summaries.append({\"location_id\": loc_id, \"location_name\": name, \"target\": tkey, \"status\":\"empty\"})\n",
    "            continue\n",
    "\n",
    "        s = {\"location_id\": loc_id, \"location_name\": name, \"target\": tkey, \"status\":\"ok\", \"n_rows\": int(len(rep))}\n",
    "        if \"test_mae\" in rep.columns:\n",
    "            s[\"test_mae_mean\"] = float(rep[\"test_mae\"].mean())\n",
    "        if \"test_rmse\" in rep.columns:\n",
    "            s[\"test_rmse_mean\"] = float(rep[\"test_rmse\"].mean())\n",
    "        if \"f1\" in rep.columns:\n",
    "            s[\"f1_mean\"] = float(rep[\"f1\"].mean())\n",
    "        summaries.append(s)\n",
    "\n",
    "# ============================================================\n",
    "# 11) SAVE RESULTS + BINS SUMMARY\n",
    "# ============================================================\n",
    "leader = pd.DataFrame(summaries)\n",
    "leader_path = REPORT_DIR / \"xgb_leaderboard.csv\"\n",
    "leader.to_csv(leader_path, index=False)\n",
    "\n",
    "# Generate bins summary\n",
    "bins_summary = summarize_all_bins()\n",
    "if len(bins_summary) > 0:\n",
    "    bins_path = REPORT_DIR / \"xgb_bins_summary.csv\"\n",
    "    bins_summary.to_csv(bins_path, index=False)\n",
    "    \n",
    "    # Aggregate bins across all locations\n",
    "    agg_bins = bins_summary.groupby([\"target\", \"horizon_bin\"]).agg({\n",
    "        \"mae_mean\": \"mean\",\n",
    "        \"rmse_mean\": \"mean\",\n",
    "    }).reset_index()\n",
    "    if \"f1_mean\" in bins_summary.columns:\n",
    "        agg_f1 = bins_summary[bins_summary[\"target\"] == \"rain\"].groupby(\"horizon_bin\")[\"f1_mean\"].mean()\n",
    "        agg_bins = agg_bins.merge(agg_f1.reset_index(), on=\"horizon_bin\", how=\"left\")\n",
    "    \n",
    "    agg_bins_path = REPORT_DIR / \"xgb_bins_aggregate.csv\"\n",
    "    agg_bins.to_csv(agg_bins_path, index=False)\n",
    "    print(\"\\nðŸ“Š BINS SUMMARY (aggregated):\")\n",
    "    print(agg_bins.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Models:\", MODEL_DIR)\n",
    "print(\"Reports:\", REPORT_DIR)\n",
    "if len(bins_summary) > 0:\n",
    "    print(\"Saved bins summary:\", bins_path)\n",
    "    print(\"Saved bins aggregate:\", agg_bins_path)\n",
    "leader"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
