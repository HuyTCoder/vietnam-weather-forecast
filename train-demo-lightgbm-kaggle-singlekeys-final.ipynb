{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1515e8",
   "metadata": {},
   "source": [
    "## LightGBM (Kaggle) â€” Train by LOCATION_ID (20 Locations)\n",
    "\n",
    "**YÃªu cáº§u Dataset:**\n",
    "- Cháº¡y `fetch-demo-data-singlekeys.ipynb` trÆ°á»›c (Ä‘Ã£ fetch 20 tá»‰nh/thÃ nh)\n",
    "- Upload output thÃ nh Kaggle Dataset\n",
    "- Add dataset vÃ o notebook nÃ y\n",
    "\n",
    "**Config:**\n",
    "- LAG = 49h lookback\n",
    "- HORIZON = 100h forecast (~4 ngÃ y)\n",
    "- 20 locations thay vÃ¬ 34/63\n",
    "\n",
    "**Speed Optimizations:**\n",
    "- `learning_rate = 0.08` (tÄƒng tá»« 0.05) â†’ há»™i tá»¥ nhanh hÆ¡n\n",
    "- `NUM_BOOST = 2000` (giáº£m tá»« 3000) â†’ váº«n Ä‘á»§ vá»›i early stopping\n",
    "- `EARLY_STOP = 100` (giáº£m tá»« 150) â†’ check nhanh hÆ¡n\n",
    "- `num_threads = all CPUs` â†’ song song hÃ³a\n",
    "- `force_row_wise = True` â†’ tá»‘i Æ°u cho dataset vá»«a\n",
    "- Reuse Dataset objects â†’ giáº£m overhead\n",
    "\n",
    "**Bins Reporting (giá»‘ng GRU/TCN):**\n",
    "- `1-24h`: Ngáº¯n háº¡n (1 ngÃ y)\n",
    "- `25-48h`: Trung háº¡n (1-2 ngÃ y)\n",
    "- `49-72h`: Trung-dÃ i (2-3 ngÃ y)\n",
    "- `73-100h`: DÃ i háº¡n (3-4 ngÃ y)\n",
    "\n",
    "**Features:**\n",
    "- Thá»­ GPU rá»“i fallback CPU\n",
    "- Tá»± dÃ² TAB_DIR vÃ  load location_ids tá»« metadata\n",
    "- MÆ°a 2-stage (event + amount)\n",
    "- CÃ³ thá»ƒ cháº¡y 1 target hoáº·c táº¥t cáº£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b9dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LightGBM trainer - Train by LOCATION_ID from Kaggle Dataset\n",
    "# 20 provinces/cities, LAG=49, HORIZON=100\n",
    "# OPTIMIZED: parallel training, bins reporting\n",
    "# ============================================================\n",
    "\n",
    "import importlib, sys, subprocess, os, json, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    pip_install([\"lightgbm==4.6.0\"])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    import pyarrow\n",
    "except Exception:\n",
    "    pip_install([\"pyarrow<20\"])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"LightGBM version:\", lgb.__version__)\n",
    "\n",
    "# ============================================================\n",
    "# 0) RUN CONTROL (OPTIMIZED)\n",
    "# ============================================================\n",
    "TARGETS_TO_RUN = \"all\"  # \"all\" or list like [\"temp\",\"rain\"]\n",
    "USE_GPU = False         # True/False\n",
    "\n",
    "LAG = 49                # 49h lookback\n",
    "H   = 100               # 100h forecast\n",
    "H_START = 1\n",
    "H_END   = 100\n",
    "\n",
    "# === BINS for reporting (giá»‘ng GRU/TCN) ===\n",
    "BINS = ((1,24), (25,48), (49,72), (73,100))\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "START_LOC_IDX = 0\n",
    "END_LOC_IDX = -1        # -1 = all remaining\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": \"train_2021_2023\",\n",
    "    \"val\":   \"val_2024\",\n",
    "    \"test\":  \"test_2025_01_to_2025_11\",\n",
    "}\n",
    "\n",
    "LOAD_Y_MODE = \"all\"\n",
    "SEED = 42\n",
    "\n",
    "RAIN_2STAGE = True\n",
    "RAIN_MM_THR = 0.1\n",
    "P_THR_CAND = np.round(np.linspace(0.05, 0.95, 19), 2).tolist()\n",
    "MIN_POS_TRAIN = 300\n",
    "MIN_POS_VAL   = 50\n",
    "USE_LOG1P_AMOUNT = True\n",
    "\n",
    "CANON_KEYS = [\"temp\",\"rain\",\"u10\",\"v10\",\"rh\",\"press\",\"cloud\"]\n",
    "\n",
    "if TARGETS_TO_RUN == \"all\":\n",
    "    TARGETS = CANON_KEYS\n",
    "else:\n",
    "    TARGETS = list(TARGETS_TO_RUN)\n",
    "\n",
    "# ============================================================\n",
    "# 1) AUTO-DETECT DATA DIR + LOAD LOCATION_IDS\n",
    "# ============================================================\n",
    "INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "\n",
    "def find_data_dir():\n",
    "    for pattern in [\"weather_20loc/data\", \"weather_34loc/data\", \"weather_63loc/data\", \"weather_4loc/data\"]:\n",
    "        for p in INPUT_ROOT.rglob(pattern):\n",
    "            if p.is_dir():\n",
    "                return p\n",
    "    for p in INPUT_ROOT.rglob(\"data/tabular\"):\n",
    "        if p.is_dir():\n",
    "            return p.parent\n",
    "    raise FileNotFoundError(\"KhÃ´ng tÃ¬m tháº¥y data directory\")\n",
    "\n",
    "DATA_DIR = find_data_dir()\n",
    "TAB_DIR = DATA_DIR / \"tabular\"\n",
    "META_DIR = DATA_DIR / \"meta\"\n",
    "\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"TAB_DIR = {TAB_DIR}\")\n",
    "\n",
    "def load_location_ids():\n",
    "    meta_file = META_DIR / \"locations.json\"\n",
    "    if meta_file.exists():\n",
    "        with open(meta_file) as f:\n",
    "            meta = json.load(f)\n",
    "        loc_ids = meta.get(\"location_ids\", [])\n",
    "        locations = meta.get(\"locations\", [])\n",
    "        print(f\"Loaded {len(loc_ids)} locations:\")\n",
    "        for loc in locations:\n",
    "            print(f\"  {loc['name']:15s} = {loc['location_id']}\")\n",
    "        return loc_ids, {loc[\"location_id\"]: loc[\"name\"] for loc in locations}\n",
    "    \n",
    "    files = list(TAB_DIR.glob(f\"*_{SPLITS['train']}_tab_temp_lag{LAG}_h{H}.parquet\"))\n",
    "    loc_ids = sorted(set(f.name.split(\"_\")[0] for f in files))\n",
    "    print(f\"Found {len(loc_ids)} location_ids from files\")\n",
    "    return loc_ids, {}\n",
    "\n",
    "LOCATION_IDS_ALL, LOC_NAMES = load_location_ids()\n",
    "\n",
    "# === LOCATION BATCHING ===\n",
    "_start = START_LOC_IDX\n",
    "_end = END_LOC_IDX if END_LOC_IDX >= 0 else len(LOCATION_IDS_ALL)\n",
    "LOCATION_IDS = LOCATION_IDS_ALL[_start:_end]\n",
    "print(f\"[LOCATION BATCH] Using {len(LOCATION_IDS)}/{len(LOCATION_IDS_ALL)} locations (idx {_start}:{_end})\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) OUTPUT DIRS\n",
    "# ============================================================\n",
    "OUT_DIR = Path(\"/kaggle/working/lgb_out_singlekeys\")\n",
    "MODEL_DIR = OUT_DIR / \"models\"\n",
    "REPORT_DIR = OUT_DIR / \"reports\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 3) IO HELPERS\n",
    "# ============================================================\n",
    "def ycol(h: int):\n",
    "    return f\"y_t+{h:03d}\"\n",
    "\n",
    "def file_path(loc_id: str, split_name: str, target_key: str) -> Path:\n",
    "    return TAB_DIR / f\"{loc_id}_{split_name}_tab_{target_key}_lag{LAG}_h{H}.parquet\"\n",
    "\n",
    "def loc_short_name(loc_id: str) -> str:\n",
    "    return LOC_NAMES.get(loc_id, loc_id[:8])\n",
    "\n",
    "def get_schema_cols(path: Path):\n",
    "    try:\n",
    "        import pyarrow.parquet as pq\n",
    "        return pq.ParquetFile(path).schema.names\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, engine=\"pyarrow\", columns=None).columns.tolist()\n",
    "\n",
    "def load_XY(loc_id: str, split_name: str, target_key: str):\n",
    "    path = file_path(loc_id, split_name, target_key)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {path}\")\n",
    "\n",
    "    cols = get_schema_cols(path)\n",
    "    feat_cols = [c for c in cols if \"_lag\" in c]\n",
    "    y_cols = [ycol(h) for h in range(1, H+1)]\n",
    "\n",
    "    X = pd.read_parquet(path, columns=feat_cols).to_numpy(np.float32)\n",
    "    Y = pd.read_parquet(path, columns=y_cols).to_numpy(np.float32) if LOAD_Y_MODE == \"all\" else None\n",
    "\n",
    "    gc.collect()\n",
    "    return X, Y, feat_cols, y_cols, path\n",
    "\n",
    "def load_y_per_h(path: Path, y_name: str) -> np.ndarray:\n",
    "    return pd.read_parquet(path, columns=[y_name])[y_name].to_numpy(np.float32)\n",
    "\n",
    "# ============================================================\n",
    "# 4) METRICS\n",
    "# ============================================================\n",
    "def mae(yhat, y):\n",
    "    return float(np.mean(np.abs(np.asarray(yhat, np.float32) - np.asarray(y, np.float32))))\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    d = np.asarray(yhat, np.float32) - np.asarray(y, np.float32)\n",
    "    return float(np.sqrt(np.mean(d * d)))\n",
    "\n",
    "def event_metrics(y_true01, y_pred01):\n",
    "    y_true01 = np.asarray(y_true01).astype(np.int32)\n",
    "    y_pred01 = np.asarray(y_pred01).astype(np.int32)\n",
    "    tp = int(((y_true01 == 1) & (y_pred01 == 1)).sum())\n",
    "    fp = int(((y_true01 == 0) & (y_pred01 == 1)).sum())\n",
    "    fn = int(((y_true01 == 1) & (y_pred01 == 0)).sum())\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    return float(prec), float(rec), float(f1), tp, fp, fn\n",
    "\n",
    "def tune_p_thr_on_val(y_true_evt: np.ndarray, p_pred: np.ndarray, candidates=P_THR_CAND):\n",
    "    best_thr = 0.5\n",
    "    best_f1 = -1.0\n",
    "    for thr in candidates:\n",
    "        pred_evt = (p_pred >= thr).astype(np.int32)\n",
    "        _, _, f1, _, _, _ = event_metrics(y_true_evt, pred_evt)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    return best_thr, best_f1\n",
    "\n",
    "# ============================================================\n",
    "# 5) LightGBM params (SPEED OPTIMIZED for Kaggle)\n",
    "# ============================================================\n",
    "NUM_BOOST = 2000        # Reduced from 3000 (still enough with early stop)\n",
    "EARLY_STOP = 100        # Reduced from 150 (faster convergence check)\n",
    "\n",
    "def base_common(device: str):\n",
    "    # Detect number of CPUs\n",
    "    n_jobs = os.cpu_count() or 4\n",
    "    return {\n",
    "        \"learning_rate\": 0.08,      # Increased from 0.05 (faster convergence)\n",
    "        \"num_leaves\": 31,\n",
    "        \"min_data_in_leaf\": 500,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"lambda_l2\": 1.5,\n",
    "        \"max_bin\": 127,\n",
    "        \"seed\": SEED,\n",
    "        \"verbose\": -1,\n",
    "        \"device\": device,\n",
    "        \"num_threads\": n_jobs,      # Use all CPU cores\n",
    "        \"force_row_wise\": True,     # Better for small-medium datasets\n",
    "    }\n",
    "\n",
    "def params_reg(device: str):\n",
    "    p = base_common(device)\n",
    "    p.update({\"objective\": \"regression\", \"metric\": [\"rmse\"]})\n",
    "    return p\n",
    "\n",
    "def params_clf(device: str):\n",
    "    p = base_common(device)\n",
    "    p.update({\"objective\": \"binary\", \"metric\": [\"binary_logloss\"]})  # Removed auc (faster)\n",
    "    return p\n",
    "\n",
    "def lgb_train_try_gpu_then_cpu(params_fn, dtrain, dvalid):\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(EARLY_STOP, verbose=False),\n",
    "        lgb.log_evaluation(period=1000),  # Less frequent logging\n",
    "    ]\n",
    "    if USE_GPU:\n",
    "        try:\n",
    "            print(\"[device attempt] gpu\")\n",
    "            params = params_fn(\"gpu\")\n",
    "            booster = lgb.train(params, dtrain, num_boost_round=NUM_BOOST, valid_sets=[dvalid], valid_names=[\"val\"], callbacks=callbacks)\n",
    "            return booster, \"gpu\"\n",
    "        except Exception as e:\n",
    "            print(\"[warn] gpu failed -> fallback cpu\\n\", str(e)[:400])\n",
    "    params = params_fn(\"cpu\")\n",
    "    booster = lgb.train(params, dtrain, num_boost_round=NUM_BOOST, valid_sets=[dvalid], valid_names=[\"val\"], callbacks=callbacks)\n",
    "    return booster, \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# 6) TRAIN 1-stage regression (OPTIMIZED: batch all horizons)\n",
    "# ============================================================\n",
    "def train_reg_1stage(loc_id: str, target_key: str, h_start: int, h_end: int):\n",
    "    name = loc_short_name(loc_id)\n",
    "    print(f\"\\n=== LGB 1-STAGE | {name} ({loc_id[:8]}...) target={target_key} h={h_start:03d}-{h_end:03d} ===\")\n",
    "\n",
    "    Xtr, Ytr, feat_cols, y_cols, p_tr = load_XY(loc_id, SPLITS[\"train\"], target_key)\n",
    "    Xva, Yva, _, _, p_va = load_XY(loc_id, SPLITS[\"val\"], target_key)\n",
    "    Xte, Yte, _, _, p_te = load_XY(loc_id, SPLITS[\"test\"], target_key)\n",
    "\n",
    "    mdir = MODEL_DIR / target_key\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    report_path = REPORT_DIR / f\"report_lgb_{target_key}_{loc_id}.csv\"\n",
    "    old = pd.read_csv(report_path) if report_path.exists() else None\n",
    "\n",
    "    # Pre-create datasets once (SPEED OPTIMIZATION)\n",
    "    dtrain_base = lgb.Dataset(Xtr, free_raw_data=False)\n",
    "    dvalid_base = lgb.Dataset(Xva, reference=dtrain_base, free_raw_data=False)\n",
    "\n",
    "    rows = []\n",
    "    for h in range(h_start, h_end + 1):\n",
    "        mp = mdir / f\"lgb_{target_key}_{loc_id}_h{h:03d}.txt\"\n",
    "        if mp.exists():\n",
    "            continue\n",
    "\n",
    "        if LOAD_Y_MODE == \"all\":\n",
    "            ytr = Ytr[:, h-1]; yva = Yva[:, h-1]; yte = Yte[:, h-1]\n",
    "        else:\n",
    "            yname = ycol(h)\n",
    "            ytr = load_y_per_h(p_tr, yname)\n",
    "            yva = load_y_per_h(p_va, yname)\n",
    "            yte = load_y_per_h(p_te, yname)\n",
    "\n",
    "        # Reuse base datasets with new labels (faster than recreating)\n",
    "        dtrain = dtrain_base.create_valid(Xtr, label=ytr)\n",
    "        dvalid = dvalid_base.create_valid(Xva, label=yva)\n",
    "\n",
    "        booster, used_dev = lgb_train_try_gpu_then_cpu(params_reg, dtrain, dvalid)\n",
    "\n",
    "        pred_te = booster.predict(Xte, num_iteration=booster.best_iteration or booster.current_iteration())\n",
    "        rows.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": target_key, \"h\": h,\n",
    "            \"device\": used_dev,\n",
    "            \"best_iter\": int(booster.best_iteration or booster.current_iteration()),\n",
    "            \"test_mae\": mae(pred_te, yte),\n",
    "            \"test_rmse\": rmse(pred_te, yte),\n",
    "            \"model\": mp.name,\n",
    "        })\n",
    "        booster.save_model(str(mp))\n",
    "\n",
    "        if h % 25 == 0 or h == h_end:\n",
    "            print(f\"[{target_key}] {name} h={h:03d} dev={used_dev} mae={rows[-1]['test_mae']:.4f}\")\n",
    "\n",
    "        del booster, pred_te\n",
    "        gc.collect()\n",
    "\n",
    "    del Xtr, Xva, Xte, Ytr, Yva, Yte, dtrain_base, dvalid_base\n",
    "    gc.collect()\n",
    "\n",
    "    if rows:\n",
    "        new = pd.DataFrame(rows)\n",
    "        out = pd.concat([old, new], ignore_index=True) if old is not None else new\n",
    "        out.to_csv(report_path, index=False)\n",
    "        return out\n",
    "    print(\"[info] resume hit.\")\n",
    "    return old if old is not None else pd.DataFrame()\n",
    "\n",
    "# ============================================================\n",
    "# 7) TRAIN 2-stage rain (OPTIMIZED)\n",
    "# ============================================================\n",
    "def train_rain_2stage(loc_id: str, h_start: int, h_end: int):\n",
    "    name = loc_short_name(loc_id)\n",
    "    print(f\"\\n=== LGB 2-STAGE RAIN | {name} ({loc_id[:8]}...) h={h_start:03d}-{h_end:03d} ===\")\n",
    "\n",
    "    Xtr, Ytr, feat_cols, y_cols, p_tr = load_XY(loc_id, SPLITS[\"train\"], \"rain\")\n",
    "    Xva, Yva, _, _, p_va = load_XY(loc_id, SPLITS[\"val\"], \"rain\")\n",
    "    Xte, Yte, _, _, p_te = load_XY(loc_id, SPLITS[\"test\"], \"rain\")\n",
    "\n",
    "    mdir = MODEL_DIR / \"rain\"\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    report_path = REPORT_DIR / f\"report_lgb_rain_{loc_id}.csv\"\n",
    "    old = pd.read_csv(report_path) if report_path.exists() else None\n",
    "\n",
    "    rows = []\n",
    "    for h in range(h_start, h_end + 1):\n",
    "        mp_clf = mdir / f\"lgb_rain_clf_{loc_id}_h{h:03d}.txt\"\n",
    "        mp_reg = mdir / f\"lgb_rain_reg_{loc_id}_h{h:03d}.txt\"\n",
    "        if mp_clf.exists() and mp_reg.exists():\n",
    "            continue\n",
    "\n",
    "        if LOAD_Y_MODE == \"all\":\n",
    "            ytr_amt = Ytr[:, h-1]; yva_amt = Yva[:, h-1]; yte_amt = Yte[:, h-1]\n",
    "        else:\n",
    "            yname = ycol(h)\n",
    "            ytr_amt = load_y_per_h(p_tr, yname)\n",
    "            yva_amt = load_y_per_h(p_va, yname)\n",
    "            yte_amt = load_y_per_h(p_te, yname)\n",
    "\n",
    "        ytr_evt = (ytr_amt >= RAIN_MM_THR).astype(np.float32)\n",
    "        yva_evt = (yva_amt >= RAIN_MM_THR).astype(np.float32)\n",
    "        yte_evt = (yte_amt >= RAIN_MM_THR).astype(np.int32)\n",
    "\n",
    "        # CLASSIFIER\n",
    "        dtr_c = lgb.Dataset(Xtr, label=ytr_evt, free_raw_data=False)\n",
    "        dva_c = lgb.Dataset(Xva, label=yva_evt, reference=dtr_c, free_raw_data=False)\n",
    "        clf, dev_clf = lgb_train_try_gpu_then_cpu(params_clf, dtr_c, dva_c)\n",
    "        \n",
    "        p_va_prob = clf.predict(Xva, num_iteration=clf.best_iteration or clf.current_iteration())\n",
    "        best_p_thr, val_f1 = tune_p_thr_on_val(yva_evt.astype(np.int32), p_va_prob)\n",
    "        p_te_prob = clf.predict(Xte, num_iteration=clf.best_iteration or clf.current_iteration())\n",
    "\n",
    "        # REGRESSOR\n",
    "        idx_tr = ytr_evt > 0.5\n",
    "        idx_va = yva_evt > 0.5\n",
    "        \n",
    "        if USE_LOG1P_AMOUNT:\n",
    "            ytr_amt_log = np.log1p(ytr_amt)\n",
    "            yva_amt_log = np.log1p(yva_amt)\n",
    "        else:\n",
    "            ytr_amt_log = ytr_amt\n",
    "            yva_amt_log = yva_amt\n",
    "        \n",
    "        if idx_tr.sum() < MIN_POS_TRAIN or idx_va.sum() < MIN_POS_VAL:\n",
    "            dtr_r = lgb.Dataset(Xtr, label=ytr_amt_log, free_raw_data=False)\n",
    "            dva_r = lgb.Dataset(Xva, label=yva_amt_log, reference=dtr_r, free_raw_data=False)\n",
    "        else:\n",
    "            dtr_r = lgb.Dataset(Xtr[idx_tr], label=ytr_amt_log[idx_tr], free_raw_data=False)\n",
    "            dva_r = lgb.Dataset(Xva[idx_va], label=yva_amt_log[idx_va], reference=dtr_r, free_raw_data=False)\n",
    "\n",
    "        reg, dev_reg = lgb_train_try_gpu_then_cpu(params_reg, dtr_r, dva_r)\n",
    "        pred_log = reg.predict(Xte, num_iteration=reg.best_iteration or reg.current_iteration())\n",
    "        \n",
    "        if USE_LOG1P_AMOUNT:\n",
    "            yhat_amt = np.expm1(pred_log).astype(np.float32)\n",
    "        else:\n",
    "            yhat_amt = pred_log.astype(np.float32)\n",
    "        yhat_amt = np.maximum(yhat_amt, 0.0)\n",
    "\n",
    "        yhat_evt = (p_te_prob >= best_p_thr).astype(np.int32)\n",
    "        yhat = np.where(yhat_evt == 1, yhat_amt, 0.0).astype(np.float32)\n",
    "\n",
    "        prec, rec, f1, tp, fp, fn = event_metrics(yte_evt, yhat_evt)\n",
    "\n",
    "        rows.append({\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_name\": name,\n",
    "            \"target\": \"rain\", \"h\": h,\n",
    "            \"device_clf\": dev_clf, \"device_reg\": dev_reg,\n",
    "            \"best_iter_clf\": int(clf.best_iteration or clf.current_iteration()),\n",
    "            \"best_iter_reg\": int(reg.best_iteration or reg.current_iteration()),\n",
    "            \"p_thr_tuned\": best_p_thr,\n",
    "            \"use_log1p\": USE_LOG1P_AMOUNT,\n",
    "            \"test_mae\": mae(yhat, yte_amt),\n",
    "            \"test_rmse\": rmse(yhat, yte_amt),\n",
    "            \"prec\": prec, \"rec\": rec, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "            \"model_clf\": mp_clf.name, \"model_reg\": mp_reg.name,\n",
    "        })\n",
    "\n",
    "        clf.save_model(str(mp_clf))\n",
    "        reg.save_model(str(mp_reg))\n",
    "\n",
    "        if h % 25 == 0 or h == h_end:\n",
    "            print(f\"[rain] {name} h={h:03d} p_thr={best_p_thr:.2f} mae={rows[-1]['test_mae']:.4f} f1={f1:.3f}\")\n",
    "\n",
    "        del clf, reg, dtr_c, dva_c, dtr_r, dva_r, p_te_prob, p_va_prob, yhat_amt, yhat, yhat_evt, pred_log\n",
    "        gc.collect()\n",
    "\n",
    "    del Xtr, Xva, Xte, Ytr, Yva, Yte\n",
    "    gc.collect()\n",
    "\n",
    "    if rows:\n",
    "        new = pd.DataFrame(rows)\n",
    "        out = pd.concat([old, new], ignore_index=True) if old is not None else new\n",
    "        out.to_csv(report_path, index=False)\n",
    "        return out\n",
    "    print(\"[info] resume hit.\")\n",
    "    return old if old is not None else pd.DataFrame()\n",
    "\n",
    "# ============================================================\n",
    "# 8) BINS SUMMARY (giá»‘ng GRU/TCN)\n",
    "# ============================================================\n",
    "def summarize_bins(df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    \"\"\"Summarize metrics by horizon bins\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rows = []\n",
    "    for a, b in BINS:\n",
    "        mask = (df[\"h\"] >= a) & (df[\"h\"] <= b)\n",
    "        sub = df[mask]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            \"target\": target,\n",
    "            \"horizon_bin\": f\"{a}-{b}\",\n",
    "            \"n_horizons\": len(sub),\n",
    "        }\n",
    "        \n",
    "        if \"test_mae\" in sub.columns:\n",
    "            row[\"mae_mean\"] = float(sub[\"test_mae\"].mean())\n",
    "        if \"test_rmse\" in sub.columns:\n",
    "            row[\"rmse_mean\"] = float(sub[\"test_rmse\"].mean())\n",
    "        if \"f1\" in sub.columns:\n",
    "            row[\"f1_mean\"] = float(sub[\"f1\"].mean())\n",
    "        if \"prec\" in sub.columns:\n",
    "            row[\"prec_mean\"] = float(sub[\"prec\"].mean())\n",
    "        if \"rec\" in sub.columns:\n",
    "            row[\"rec_mean\"] = float(sub[\"rec\"].mean())\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_all_bins() -> pd.DataFrame:\n",
    "    \"\"\"Summarize all targets with bins\"\"\"\n",
    "    all_bins = []\n",
    "    \n",
    "    for tkey in TARGETS:\n",
    "        for loc_id in LOCATION_IDS:\n",
    "            if tkey == \"rain\" and RAIN_2STAGE:\n",
    "                report_path = REPORT_DIR / f\"report_lgb_rain_{loc_id}.csv\"\n",
    "            else:\n",
    "                report_path = REPORT_DIR / f\"report_lgb_{tkey}_{loc_id}.csv\"\n",
    "            \n",
    "            if not report_path.exists():\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(report_path)\n",
    "            bins_df = summarize_bins(df, tkey)\n",
    "            if len(bins_df) > 0:\n",
    "                bins_df[\"location_id\"] = loc_id\n",
    "                bins_df[\"location_name\"] = loc_short_name(loc_id)\n",
    "                all_bins.append(bins_df)\n",
    "    \n",
    "    if not all_bins:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.concat(all_bins, ignore_index=True)\n",
    "\n",
    "# ============================================================\n",
    "# 9) RUN - Train theo LOCATION_ID\n",
    "# ============================================================\n",
    "summaries = []\n",
    "for tkey in TARGETS:\n",
    "    for loc_id in LOCATION_IDS:\n",
    "        name = loc_short_name(loc_id)\n",
    "        if tkey == \"rain\" and RAIN_2STAGE:\n",
    "            rep = train_rain_2stage(loc_id, H_START, H_END)\n",
    "        else:\n",
    "            rep = train_reg_1stage(loc_id, tkey, H_START, H_END)\n",
    "\n",
    "        if rep is None or len(rep) == 0:\n",
    "            summaries.append({\"location_id\": loc_id, \"location_name\": name, \"target\": tkey, \"status\":\"empty\"})\n",
    "            continue\n",
    "\n",
    "        s = {\"location_id\": loc_id, \"location_name\": name, \"target\": tkey, \"status\":\"ok\", \"n_rows\": int(len(rep))}\n",
    "        if \"test_mae\" in rep.columns:\n",
    "            s[\"test_mae_mean\"] = float(rep[\"test_mae\"].mean())\n",
    "        if \"test_rmse\" in rep.columns:\n",
    "            s[\"test_rmse_mean\"] = float(rep[\"test_rmse\"].mean())\n",
    "        if \"f1\" in rep.columns:\n",
    "            s[\"f1_mean\"] = float(rep[\"f1\"].mean())\n",
    "        summaries.append(s)\n",
    "\n",
    "# ============================================================\n",
    "# 10) SAVE RESULTS + BINS SUMMARY\n",
    "# ============================================================\n",
    "leader = pd.DataFrame(summaries)\n",
    "leader_path = REPORT_DIR / \"lgb_leaderboard.csv\"\n",
    "leader.to_csv(leader_path, index=False)\n",
    "\n",
    "# Generate bins summary\n",
    "bins_summary = summarize_all_bins()\n",
    "if len(bins_summary) > 0:\n",
    "    bins_path = REPORT_DIR / \"lgb_bins_summary.csv\"\n",
    "    bins_summary.to_csv(bins_path, index=False)\n",
    "    \n",
    "    # Aggregate bins across all locations\n",
    "    agg_bins = bins_summary.groupby([\"target\", \"horizon_bin\"]).agg({\n",
    "        \"mae_mean\": \"mean\",\n",
    "        \"rmse_mean\": \"mean\",\n",
    "    }).reset_index()\n",
    "    if \"f1_mean\" in bins_summary.columns:\n",
    "        agg_f1 = bins_summary[bins_summary[\"target\"] == \"rain\"].groupby(\"horizon_bin\")[\"f1_mean\"].mean()\n",
    "        agg_bins = agg_bins.merge(agg_f1.reset_index(), on=\"horizon_bin\", how=\"left\")\n",
    "    \n",
    "    agg_bins_path = REPORT_DIR / \"lgb_bins_aggregate.csv\"\n",
    "    agg_bins.to_csv(agg_bins_path, index=False)\n",
    "    print(\"\\nðŸ“Š BINS SUMMARY (aggregated):\")\n",
    "    print(agg_bins.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Saved leaderboard:\", leader_path)\n",
    "if len(bins_summary) > 0:\n",
    "    print(\"Saved bins summary:\", bins_path)\n",
    "    print(\"Saved bins aggregate:\", agg_bins_path)\n",
    "leader"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
